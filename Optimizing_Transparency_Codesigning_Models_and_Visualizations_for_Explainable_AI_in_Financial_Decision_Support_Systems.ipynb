{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Library Preparation**"
      ],
      "metadata": {
        "id": "JMeZ9kx2TnwK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gEa1q38svQjS"
      },
      "outputs": [],
      "source": [
        "!pip install squarify\n",
        "!pip install --upgrade scikit-learn\n",
        "!pip install shapely\n",
        "!pip install ribs\n",
        "!pip install --upgrade sqlalchemy\n",
        "!pip install --upgrade typing-extensions==4.5.0\n",
        "!pip install --upgrade plotly\n",
        "import sys\n",
        "!{sys.executable} -m pip install interpret\n",
        "!pip install pycaret[full]\n",
        "!pip install dash\n",
        "!pip install jupyter-dash\n",
        "!pip install pycaret\n",
        "from pycaret.classification import *\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "import sklearn\n",
        "from itertools import chain\n",
        "#import matplotlib as plt\n",
        "from pandas.plotting import parallel_coordinates\n",
        "import seaborn as sns\n",
        "from interpret import set_visualize_provider\n",
        "from interpret.provider import InlineProvider\n",
        "set_visualize_provider(InlineProvider())\n",
        "from skimage.morphology import closing, square\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.pipeline import Pipeline\n",
        "import json\n",
        "import matplotlib.colors as mcolors\n",
        "import matplotlib.patches as patches\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.pipeline import Pipeline\n",
        "from interpret import show\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import matplotlib.patches as mpatches\n",
        "from scipy.spatial import distance\n",
        "from skimage.morphology import closing, square\n",
        "from skimage.measure import label, regionprops\n",
        "from sklearn import tree\n",
        "from matplotlib import cm\n",
        "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
        "from sklearn.manifold import MDS\n",
        "from sklearn import metrics\n",
        "from scipy.ndimage.morphology import binary_closing\n",
        "from skimage import measure\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.svm import LinearSVC\n",
        "import copy\n",
        "from skimage.feature import peak_local_max\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy import ndimage\n",
        "from skimage.segmentation import watershed\n",
        "import matplotlib.colors\n",
        "from skimage.morphology import closing, square\n",
        "from scipy.integrate import simps\n",
        "from scipy.integrate import dblquad\n",
        "import matplotlib.colors as mcolors\n",
        "import math\n",
        "from matplotlib import path\n",
        "import matplotlib.patches as mpatches\n",
        "from scipy.spatial import distance\n",
        "from skimage.morphology import closing, square\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from skimage.measure import label, regionprops\n",
        "import squarify\n",
        "from matplotlib.patches import Polygon\n",
        "from sklearn.neighbors import KernelDensity\n",
        "import matplotlib.collections\n",
        "import imageio\n",
        "import skimage.feature\n",
        "from shapely.geometry import Polygon\n",
        "from matplotlib.colors import ListedColormap\n",
        "from matplotlib import colors\n",
        "import skimage.color\n",
        "from scipy.ndimage import gaussian_filter\n",
        "from skimage import measure\n",
        "from sklearn import tree\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report, confusion_matrix\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import matplotlib.patches as patches\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.inspection import PartialDependenceDisplay, partial_dependence, permutation_importance\n",
        "from scipy.stats import gaussian_kde\n",
        "from pickle import TRUE\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, HTML, Image, clear_output\n",
        "from PIL import Image as PILImage, ImageDraw, ImageChops\n",
        "import base64\n",
        "import io\n",
        "import pdb\n",
        "import math\n",
        "import time\n",
        "import random\n",
        "from pathlib import Path\n",
        "import tqdm\n",
        "from scipy.spatial.distance import pdist, squareform, cdist\n",
        "from ribs.archives import CVTArchive, GridArchive\n",
        "from ribs.emitters import (EvolutionStrategyEmitter, GaussianEmitter,\n",
        "                           GradientArborescenceEmitter, IsoLineEmitter)\n",
        "from ribs.schedulers import Scheduler\n",
        "from ribs.visualize import cvt_archive_heatmap, grid_archive_heatmap\n",
        "from interpret.blackbox import PartialDependence\n",
        "from google.colab import drive\n",
        "import os\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import plotly.express as px\n",
        "from dash import Dash, dcc, html\n",
        "from dash import Dash, dcc, html, Input, Output, State\n",
        "from dash.dependencies import Input, Output, State\n",
        "import gc\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dataset Choice**"
      ],
      "metadata": {
        "id": "rxKAUhR-Tui4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5IjCftSHLOdp",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Mount your drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Change the working directory to the folder you want to use\n",
        "os.chdir('/content/drive/MyDrive/datasets') # Insert the path of the folder here\n",
        "\n",
        "# Create a list with the names of the files in the folder\n",
        "# List only CSV files in the directory\n",
        "csv_files = [file for file in os.listdir() if file.endswith('.csv')]\n",
        "\n",
        "# Create a widget for the dropdown menu\n",
        "dropdown = widgets.Dropdown(\n",
        "    options=csv_files,\n",
        "    description=\"Dataset:\",\n",
        "    value=csv_files[0]\n",
        ")\n",
        "\n",
        "# Create a function that reads the selected dataset and displays it\n",
        "def show_dataset(file):\n",
        "    global df, l_dict, label_dict\n",
        "    # Read the dataset using pandas\n",
        "    df = pd.read_csv(file)\n",
        "\n",
        "    if os.path.isfile(file[:-4]+'_dict.json'):\n",
        "        with open(file[:-4]+'_dict.json', \"r\") as f:\n",
        "            label_dict = json.load(f)\n",
        "        l_dict = True\n",
        "    else:\n",
        "        l_dict = False\n",
        "\n",
        "    threshold = 0.5\n",
        "    i = 0\n",
        "    while i < df.shape[1]:\n",
        "        if df[df.columns[i]].isnull().mean()*100 > threshold:\n",
        "            df.drop(df.columns[i], axis=1, inplace=True)\n",
        "            gc.collect()\n",
        "        else:\n",
        "            df.dropna(subset=[df.columns[i]], inplace=True)\n",
        "            gc.collect()\n",
        "            i+=1\n",
        "    # Display the dataset\n",
        "    display(df)\n",
        "\n",
        "# Create a widget for the button\n",
        "button = widgets.Button(\n",
        "    description=\"Show the selected dataset\"\n",
        ")\n",
        "\n",
        "# Create a function that executes the show_dataset function when the button is clicked\n",
        "def on_button_clicked(b):\n",
        "    # Get the value from the dropdown menu\n",
        "    file = dropdown.value\n",
        "    # Execute the show_dataset function\n",
        "    show_dataset(file)\n",
        "\n",
        "# Assign the on_button_clicked function to the button\n",
        "button.on_click(on_button_clicked)\n",
        "\n",
        "# Display the dropdown menu and the button widget\n",
        "display(dropdown, button)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Target Choice**"
      ],
      "metadata": {
        "id": "z_hkA1I0T0Tx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJQj4tgYqzgN"
      },
      "outputs": [],
      "source": [
        "# Import the ipywidgets module\n",
        "import ipywidgets as widgets\n",
        "\n",
        "# Create a list with the names of the input dataset columns\n",
        "columns = list(df.columns)\n",
        "\n",
        "# Create a dropdown menu widget\n",
        "dropdown = widgets.Dropdown(\n",
        "    options=columns,  # Use column names as options\n",
        "    value=columns[0],  # Use the 1st column as the default value\n",
        "    description='Target:',  # The description that appears before the menu\n",
        "    disabled=False  # If set to True, the menu is not editable\n",
        ")\n",
        "\n",
        "# Create a function that assigns the value of the dropdown menu to the variable target_column\n",
        "def set_target_column(b):\n",
        "    # Get the value of the dropdown menu\n",
        "    global target_column  # Use the global keyword to make the target_column variable accessible outside the function\n",
        "    target_column = dropdown.value\n",
        "    # Print a confirmation message\n",
        "    print(f\"You have chosen the column {target_column} as the target.\")\n",
        "\n",
        "# Create a button widget\n",
        "button = widgets.Button(\n",
        "    description=\"Confirm\"  # The description that appears on the button\n",
        ")\n",
        "\n",
        "# Assign the set_target_column function to the button\n",
        "button.on_click(set_target_column)\n",
        "\n",
        "# Display the dropdown menu and button widgets\n",
        "display(dropdown, button)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Best Model**"
      ],
      "metadata": {
        "id": "G8Zidon2T6Ql"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wUjwb1nFmOkG"
      },
      "outputs": [],
      "source": [
        "#num_folds=2\n",
        "\n",
        "# initialize the environment and perform some preprocessing steps\n",
        "clf1 = setup(data = df, target = target_column, train_size = 0.8)\n",
        "\n",
        "# start a loop\n",
        "models = []\n",
        "results = []\n",
        "\n",
        "for i in np.arange(0.1,1,0.1):\n",
        "    model = create_model('lightgbm', learning_rate = i)\n",
        "    model_results = pull().loc[['Mean']]\n",
        "    models.append(model)\n",
        "    results.append(model_results)\n",
        "\n",
        "results = pd.concat(results, axis=0)\n",
        "results.index = np.arange(0.1,1,0.1)\n",
        "results.plot()\n",
        "\n",
        "# compare models\n",
        "best = compare_models()\n",
        "\n",
        "# evaluate model\n",
        "evaluate_model(best)\n",
        "\n",
        "# plot feature importance\n",
        "plot_model(best, plot = 'feature')\n",
        "\n",
        "# create model\n",
        "model = create_model(best)\n",
        "\n",
        "# finalize model\n",
        "pipeline = finalize_model(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DFA3HJFxk6TP"
      },
      "outputs": [],
      "source": [
        "# get the train and test data\n",
        "X_train_transformed = get_config('X_train_transformed')\n",
        "y_train_transformed = get_config('y_train_transformed')\n",
        "X_test_transformed = get_config('X_test_transformed')\n",
        "y_test_transformed = get_config('y_test_transformed')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ZSa_EyAtlTR"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    # use the feature importance attribute of the model\n",
        "    feature_importance = pd.DataFrame({'Feature': X_test_transformed.columns, 'Importance': best.feature_importances_})\n",
        "    top_four_features = feature_importance.nlargest(4, 'Importance')['Feature'].tolist()\n",
        "except AttributeError:\n",
        "    # use the permutation feature importance method\n",
        "    result = permutation_importance(model, X_test_transformed, y_test_transformed, n_repeats=10, random_state=0)\n",
        "    feature_importance = pd.DataFrame({'Feature': X_test_transformed.columns, 'Importance': result.importances_mean})\n",
        "    # Assuming 'feature_importance' is a DataFrame with columns 'Feature' and 'Importance'\n",
        "    top_four_features = feature_importance.nlargest(4, 'Importance')['Feature'].tolist()\n",
        "\n",
        "# Plot the bar chart using the correct variable\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(top_four_features, feature_importance.nlargest(4, 'Importance')['Importance'])\n",
        "plt.xlabel('Feature')\n",
        "plt.ylabel('Importance')\n",
        "plt.title('Top Four Features Importance')\n",
        "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Extract feature names\n",
        "feature1 = top_four_features[0]\n",
        "feature2 = top_four_features[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Plotting graphs and Gestalt Metrics computation**"
      ],
      "metadata": {
        "id": "iHZgZNwBUBiE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7PFKXOeYcFg"
      },
      "source": [
        "##**Symmetry Calculation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qaav_vLr0Uzr"
      },
      "outputs": [],
      "source": [
        "def calculate_symmetry(image_path):\n",
        "    # Open the image\n",
        "    img = imageio.imread(image_path)\n",
        "    # Remove the alpha channel\n",
        "    image_rgb = img[..., :3]\n",
        "\n",
        "    # Convert the image to grayscale\n",
        "    image_gray = skimage.color.rgb2gray(image_rgb)\n",
        "\n",
        "    # Get the height and width of the grayscale image\n",
        "    height, width = np.shape(image_gray)\n",
        "\n",
        "    # Set a threshold value\n",
        "    thrld = 0.1\n",
        "\n",
        "    # Invert the grayscale image\n",
        "    new_gray = 1. - image_gray\n",
        "    new_gray = np.where(new_gray <= thrld, 0, new_gray)\n",
        "\n",
        "    # Calculate the sum of pixel values in the thresholded grayscale image\n",
        "    val_img = np.sum(new_gray)\n",
        "\n",
        "    # Calculate vertical symmetry score\n",
        "    left_half = new_gray[:, :width // 2].copy()\n",
        "\n",
        "    right_half = new_gray[:, -(width // 2):].copy()\n",
        "\n",
        "    # Create a mirror image of the right half\n",
        "    right_half_flipped = np.fliplr(right_half)\n",
        "\n",
        "    # Compute the absolute difference between left and flipped right halves\n",
        "    diff = np.abs(left_half - right_half_flipped)\n",
        "\n",
        "    # Threshold the difference image\n",
        "    new_diff = np.where(diff <= thrld, 0, diff)\n",
        "\n",
        "    # Calculate the vertical symmetry score\n",
        "    vertical_score = 1 - (np.sum(new_diff) / val_img)\n",
        "\n",
        "    # Calculate horizontal symmetry score\n",
        "    top_half = new_gray[:height//2 , :].copy()\n",
        "\n",
        "    bottom_half = new_gray[-(height//2): , :].copy()\n",
        "\n",
        "    bottom_half_flipped = np.flipud(bottom_half)\n",
        "\n",
        "    diff = np.abs(top_half-bottom_half_flipped)\n",
        "    new_diff = np.where(diff <= thrld, 0 , diff)\n",
        "\n",
        "    horizontal_score = 1 -(np.sum(new_diff) / val_img)\n",
        "\n",
        "    # Calculate main oblique symmetry score\n",
        "    image = PILImage.open(image_path)\n",
        "\n",
        "    # Calculates the angle of rotation so that the diagonal of the graph is horizontal\n",
        "    rad_angle = math.atan(height/width)\n",
        "    grad_angle = -math.degrees(rad_angle)\n",
        "    # Rotate original image\n",
        "    oblique = image.rotate(grad_angle, expand=True)\n",
        "\n",
        "    # Create a new white background with the same size as the rotated image\n",
        "    white_background = PILImage.new(\"RGB\", oblique.size, \"white\")\n",
        "\n",
        "    # Overlay the rotated image on the new white background\n",
        "    result_image = PILImage.alpha_composite(white_background.convert(\"RGBA\"), oblique.convert(\"RGBA\"))\n",
        "    oimg = result_image.convert(\"L\")\n",
        "\n",
        "    # Convert image to greyscale\n",
        "    image_gray = np.array(oimg)\n",
        "    image_gray = 1. - image_gray / 255\n",
        "\n",
        "    height_r, width_r = np.shape(image_gray)\n",
        "\n",
        "    new_gray = np.where(image_gray <= thrld, 0 , image_gray)\n",
        "    val_img = np.sum(new_gray)\n",
        "\n",
        "    top_half = new_gray[:height_r//2 , :].copy()\n",
        "\n",
        "    bottom_half = new_gray[-(height_r//2): , :].copy()\n",
        "\n",
        "    bottom_half_flipped = np.flipud(bottom_half)\n",
        "\n",
        "    diff = np.abs(top_half-bottom_half_flipped)\n",
        "\n",
        "    new_diff = np.where(diff <= thrld, 0 , diff)\n",
        "\n",
        "    oblique_m_score = 1 - (np.sum(new_diff) / val_img)\n",
        "\n",
        "    # Calculate secondary oblique symmetry score\n",
        "    grad_angle = -grad_angle\n",
        "    # Rotate original image\n",
        "    oblique = image.rotate(grad_angle, expand=True)\n",
        "\n",
        "    # Create a new white background with the same size as the rotated image\n",
        "    white_background = PILImage.new(\"RGB\", oblique.size, \"white\")\n",
        "\n",
        "    # Overlay the rotated image on the new white background\n",
        "    result_image = PILImage.alpha_composite(white_background.convert(\"RGBA\"), oblique.convert(\"RGBA\"))\n",
        "\n",
        "    # Convert image to greyscale\n",
        "    oimg = result_image.convert(\"L\")\n",
        "\n",
        "    image_gray = np.array(oimg)\n",
        "    image_gray = 1. - image_gray / 255\n",
        "\n",
        "    new_gray = np.where(image_gray <= thrld, 0 , image_gray)\n",
        "    val_img = np.sum(new_gray)\n",
        "\n",
        "    top_half = new_gray[:height_r//2 , :].copy()\n",
        "    height_h, width_h  = np.shape(top_half)\n",
        "\n",
        "    bottom_half = new_gray[-(height_r//2): , :].copy()\n",
        "\n",
        "    bottom_half_flipped = np.flipud(bottom_half)\n",
        "\n",
        "    diff = np.abs(top_half-bottom_half_flipped)\n",
        "\n",
        "    new_diff = np.where(diff <= thrld, 0 , diff)\n",
        "\n",
        "    oblique_s_score = 1 - (np.sum(new_diff) / val_img)\n",
        "\n",
        "    # Combine all scores into a single score\n",
        "    combined_score = (vertical_score + horizontal_score + oblique_m_score + oblique_s_score) / 4\n",
        "\n",
        "    return combined_score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Overlap Calculation**"
      ],
      "metadata": {
        "id": "ksq2dqg2UIrg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfcJ5vDnbI8z"
      },
      "outputs": [],
      "source": [
        "def calculate_overlap(accepted, refused, background_a, background_r):\n",
        "    # Open image for accepted\n",
        "    image = PILImage.open(accepted)\n",
        "    img = np.array(image, dtype=np.int16)\n",
        "    # Remove the alpha channel\n",
        "    rgb_accepted = img[..., :3]\n",
        "    height, width, _ = np.shape(rgb_accepted)\n",
        "    perc = 0.1\n",
        "\n",
        "    # Open image for refused\n",
        "    image = PILImage.open(refused)\n",
        "    img = np.array(image, dtype=np.int16)\n",
        "    # Remove the alpha channel\n",
        "    rgb_refused = img[..., :3]\n",
        "\n",
        "   # Based on the background colour, it selects points excluding those within the threshold defined by perc for accepted\n",
        "    if background_a == 'black':\n",
        "        lower_bound_a = np.array([[0, 0, 0]], dtype=np.int16)\n",
        "        upper_bound_a =np.array([[math.floor(perc * 255), math.floor(perc * 255), math.floor(perc * 255)]], dtype=np.int16)\n",
        "    elif background_a == 'blue':\n",
        "        rgb_accepted_array = np.reshape(rgb_accepted, (-1, 3))\n",
        "        brg_accepted_array = np.column_stack((rgb_accepted_array[:, 2], rgb_accepted_array[:, 0], rgb_accepted_array[:, 1]))\n",
        "\n",
        "        # Creates a dictionary of unique tuples\n",
        "        unique_accepted = np.unique(brg_accepted_array, axis=0)\n",
        "        unique_dict_accepted = {i: unique_accepted[i] for i in range(len(unique_accepted))}\n",
        "\n",
        "        list_accepted = list(unique_dict_accepted.items())\n",
        "        list_accepted=[tup[1] for tup in list_accepted]\n",
        "\n",
        "        lim=math.ceil(len(list_accepted)*perc)\n",
        "        lim_blue = list_accepted[-lim:]\n",
        "        b = [tup[0] for tup in lim_blue]\n",
        "        b = np.array(b)\n",
        "\n",
        "        r = [tup[1] for tup in lim_blue]\n",
        "        r = np.array(r)\n",
        "\n",
        "        g = [tup[2] for tup in lim_blue]\n",
        "        g = np.array(g)\n",
        "\n",
        "        lower_bound_a = np.array([[r.min(), g.min(), b.min()]], dtype=np.int16)\n",
        "        upper_bound_a = np.array([[255, 255, 255]], dtype=np.int16)\n",
        "\n",
        "    elif background_a == 'red':\n",
        "        rgb_accepted_array = np.reshape(rgb_accepted, (-1, 3))\n",
        "        rbg_accepted_array = np.column_stack((rgb_accepted_array[:, 0], rgb_accepted_array[:, 2], rgb_accepted_array[:, 1]))\n",
        "\n",
        "        # Creates a dictionary of unique tuples\n",
        "        unique_accepted = np.unique(rbg_accepted_array, axis=0)\n",
        "        unique_dict_accepted = {i: unique_accepted[i] for i in range(len(unique_accepted))}\n",
        "\n",
        "        list_accepted = list(unique_dict_accepted.items())\n",
        "        list_accepted=[tup[1] for tup in list_accepted]\n",
        "\n",
        "        lim=math.ceil(len(list_accepted)*perc)\n",
        "        lim_red = list_accepted[-lim:]\n",
        "        r = [tup[0] for tup in lim_red]\n",
        "        r = np.array(r)\n",
        "\n",
        "        b = [tup[1] for tup in lim_red]\n",
        "        b = np.array(b)\n",
        "\n",
        "        g = [tup[2] for tup in lim_red]\n",
        "        g = np.array(g)\n",
        "\n",
        "        lower_bound_a = np.array([[r.min(), g.min(), b.min()]], dtype=np.int16)\n",
        "        upper_bound_a = np.array([[255, 255, 255]], dtype=np.int16)\n",
        "\n",
        "    else:\n",
        "        lower_bound_a = np.array([[math.floor((1-perc) * 255), math.floor((1-perc) * 255), math.floor((1-perc) * 255)]], dtype=np.int16)\n",
        "        upper_bound_a = np.array([[255, 255, 255]], dtype=np.int16)\n",
        "\n",
        "   # Based on the background colour, it selects points excluding those within the threshold defined by perc for refused\n",
        "    if background_r == 'black':\n",
        "        lower_bound_r = np.array([[0, 0, 0]], dtype=np.int16)\n",
        "        upper_bound_r = np.array([[math.floor(perc * 255), math.floor(perc * 255), math.floor(perc * 255)]], dtype=np.int16)\n",
        "    elif background_r == 'blue':\n",
        "        rgb_rejected_array = np.reshape(rgb_refused, (-1, 3))\n",
        "        brg_rejected_array = np.column_stack((rgb_rejected_array[:, 2], rgb_rejected_array[:, 0], rgb_rejected_array[:, 1]))\n",
        "\n",
        "        # Create a dictionary of unique tuples\n",
        "        unique_rejected = np.unique(brg_rejected_array, axis=0)\n",
        "        unique_dict_rejected = {i: unique_accepted[i] for i in range(len(unique_accepted))}\n",
        "\n",
        "        list_rejected = list(unique_dict_rejected.items())\n",
        "        list_rejected=[tup[1] for tup in list_rejected]\n",
        "\n",
        "        lim=math.ceil(len(list_rejected)*perc)\n",
        "        lim_blue = list_rejected[-lim:]\n",
        "        b = [tup[0] for tup in lim_blue]\n",
        "        b = np.array(b)\n",
        "\n",
        "        r = [tup[1] for tup in lim_blue]\n",
        "        r = np.array(r)\n",
        "\n",
        "        g = [tup[2] for tup in lim_blue]\n",
        "        g = np.array(g)\n",
        "\n",
        "        lower_bound_r = np.array([[r.min(), g.min(), b.min()]], dtype=np.int16)\n",
        "        upper_bound_r = np.array([[255, 255, 255]], dtype=np.int16)\n",
        "\n",
        "    elif background_r == 'red':\n",
        "        rgb_rejected_array = np.reshape(rgb_refused, (-1, 3))\n",
        "        rbg_rejected_array = np.column_stack((rgb_rejected_array[:, 0], rgb_rejected_array[:, 2], rgb_rejected_array[:, 1]))\n",
        "\n",
        "        unique_rejected = np.unique(rbg_rejected_array, axis=0)\n",
        "        unique_dict_rejected = {i: unique_rejected[i] for i in range(len(unique_rejected))}\n",
        "\n",
        "        list_rejected = list(unique_dict_rejected.items())\n",
        "        list_rejected=[tup[1] for tup in list_rejected]\n",
        "\n",
        "        lim=math.ceil(len(list_rejected)*perc)\n",
        "        lim_red = list_rejected[-lim:]\n",
        "        r = [tup[0] for tup in lim_red]\n",
        "        r = np.array(r)\n",
        "\n",
        "        b = [tup[1] for tup in lim_red]\n",
        "        b = np.array(b)\n",
        "\n",
        "        g = [tup[2] for tup in lim_red]\n",
        "        g = np.array(g)\n",
        "\n",
        "        lower_bound_r = np.array([[r.min(), g.min(), b.min()]], dtype=np.int16)\n",
        "        upper_bound_r = np.array([[255, 255, 255]], dtype=np.int16)\n",
        "\n",
        "    else:\n",
        "        lower_bound_r = np.array([[math.floor((1-perc) * 255), math.floor((1-perc) * 255), math.floor((1-perc) * 255)]], dtype=np.int16)\n",
        "        upper_bound_r = np.array([[255, 255, 255]], dtype=np.int16)\n",
        "\n",
        "    # Calculate\n",
        "    # Take the average difference across the red and green color channels\n",
        "    # Verify the cells that belong to the background (true)\n",
        "    condition_bound_accepted = np.all((lower_bound_a <= rgb_accepted) & (rgb_accepted <= upper_bound_a), axis=-1)\n",
        "    # Add a dimension for the correct transmission\n",
        "    condition_bound_accepted = np.expand_dims(condition_bound_accepted, axis=-1)\n",
        "\n",
        "    condition_bound_refused = np.all((lower_bound_r <= rgb_refused) & (rgb_refused <= upper_bound_r), axis=-1)\n",
        "    condition_bound_refused = np.expand_dims(condition_bound_refused, axis=-1)\n",
        "    # Compute the values of the pixels that do not belong to the background and that can be overlapped\n",
        "    max_overlap = min(np.count_nonzero(~(condition_bound_accepted)), np.count_nonzero(~(condition_bound_refused)))\n",
        "\n",
        "    overlap = 1 - np.count_nonzero(~(condition_bound_accepted | condition_bound_refused)) / max_overlap\n",
        "\n",
        "    return  overlap"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Closure Calculation**"
      ],
      "metadata": {
        "id": "YVFjxvBQUcoI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EmX-yoWoOsvN"
      },
      "outputs": [],
      "source": [
        "def closure_1(graph_obj, frequencies, frequencies_unique):\n",
        "    # Initialise area size list\n",
        "    areas = []\n",
        "    # Set percentiles for frequencies to group similar nuances\n",
        "    percentili = [0, 20, 50, 100]\n",
        "    freq_sel = 0\n",
        "\n",
        "    for i in range(len(percentili)):\n",
        "        # If 0 I calculate the maximum area\n",
        "        if i == 0:\n",
        "            new_frequencies = np.ones_like(frequencies) * frequencies.max()\n",
        "        else:\n",
        "            low_limit = freq_sel+1\n",
        "            up_limit = int(np.percentile(frequencies_unique,percentili[i]))\n",
        "            freq_sel=up_limit\n",
        "            new_frequencies = np.where((frequencies >= low_limit) & (frequencies <= up_limit), frequencies.max(), 0)\n",
        "        graph_obj.set_array(new_frequencies)\n",
        "        buffer = io.BytesIO()\n",
        "        # I save the graph to a temporary memory\n",
        "        plt.savefig(buffer, format='png', bbox_inches='tight')\n",
        "        buffer.seek(0)\n",
        "\n",
        "        image = imageio.imread(buffer)\n",
        "        buffer.close()\n",
        "\n",
        "        # Remove the alpha channel\n",
        "        image_rgb = image[..., :3]\n",
        "\n",
        "        # Convert the image to grayscale\n",
        "        image_gray = skimage.color.rgb2gray(image_rgb)\n",
        "\n",
        "        edges = skimage.feature.canny(\n",
        "            image=image_gray,\n",
        "            sigma=0.1,\n",
        "            low_threshold=0.1,\n",
        "            high_threshold=0.2,\n",
        "        )\n",
        "        closed_edges = binary_closing(edges)\n",
        "        labels = measure.label(closed_edges)\n",
        "\n",
        "        # Get the grayscale value of the white color (assuming it's 1.0)\n",
        "        non_white = 0.5\n",
        "\n",
        "        # Create a mask for the white regions\n",
        "        white_mask = np.where(image_gray >= non_white, 1, 0)\n",
        "\n",
        "        # Exclude the white regions from the labels\n",
        "        labels = labels * (1 - white_mask)    # Label the distinct areas\n",
        "\n",
        "        # Measure the size of each area\n",
        "        # Count the number of distinct areas\n",
        "        num_areas = labels.max()\n",
        "\n",
        "\n",
        "        # Measure the size of each area\n",
        "        props = measure.regionprops(labels)\n",
        "        sizes = [prop.area for prop in props]\n",
        "\n",
        "        if i == 0:\n",
        "            max_area = sizes[0]\n",
        "        else:\n",
        "            areas.append(sizes)\n",
        "            total_area = sum(sizes)\n",
        "\n",
        "    areas_list = list(chain(*areas))\n",
        "    return areas_list, max_area"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mYH79D8T82wy"
      },
      "outputs": [],
      "source": [
        "def closure_2(path):\n",
        "    image = imageio.imread(path)\n",
        "\n",
        "    # Remove the alpha channel\n",
        "    image_rgb = image[..., :3]\n",
        "\n",
        "    # Convert the image to grayscale\n",
        "    image_gray = skimage.color.rgb2gray(image_rgb)\n",
        "\n",
        "    edges = skimage.feature.canny(\n",
        "        image=image_gray,\n",
        "        sigma=0.05,\n",
        "        low_threshold=0.1,\n",
        "        high_threshold=0.3,\n",
        "    )\n",
        "    closed_edges = binary_closing(edges)\n",
        "    labels = measure.label(closed_edges)\n",
        "\n",
        "    # Get the grayscale value of the white color (assuming it's 1.0)\n",
        "    non_white = 0.5\n",
        "\n",
        "    # Create a mask for the white regions\n",
        "    white_mask = np.where(image_gray >= non_white, 1, 0)\n",
        "\n",
        "    # Exclude the white regions from the labels\n",
        "    labels = labels * (1 - white_mask)    # Label the distinct areas\n",
        "\n",
        "    # Measure the size of each area\n",
        "    # Count the number of distinct areas\n",
        "    num_areas = labels.max()\n",
        "\n",
        "\n",
        "    # Measure the size of each area\n",
        "    props = measure.regionprops(labels)\n",
        "    sizes = [prop.area for prop in props]\n",
        "\n",
        "    return sizes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Proximity Calculation**"
      ],
      "metadata": {
        "id": "ORt2p6gGUgTw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbyZAxFcQNrw"
      },
      "outputs": [],
      "source": [
        "def proximity_1(verts, frequency, perc):\n",
        "\n",
        "    # Calculate the frequency relative to the percentile perc to exclude figures that are too faded\n",
        "    min_density = np.percentile(frequency, perc)\n",
        "\n",
        "    # Filter indices based on density\n",
        "    valid_index = np.where(frequency > min_density)[0]\n",
        "\n",
        "    # Select valid figure indices\n",
        "    valid_verts = verts[valid_index]\n",
        "\n",
        "    # Initialising the list of distances\n",
        "    distances = []\n",
        "\n",
        "    # Set the number of iterations\n",
        "    cycle = valid_verts.shape[0] - 1\n",
        "\n",
        "    for i in range(cycle):\n",
        "\n",
        "        # Fit the model\n",
        "        nbrs = NearestNeighbors(n_neighbors=2, algorithm='ball_tree').fit(valid_verts)\n",
        "\n",
        "        # Compute distances and indices of nearest neighbors\n",
        "        distance, indices = nbrs.kneighbors(valid_verts)\n",
        "\n",
        "        # The nearest distance of the first figure is added to the list of distances\n",
        "        distances.append(distance[0][1])\n",
        "\n",
        "        # Eliminate the first centre\n",
        "        valid_verts = valid_verts[1:]\n",
        "    return distances"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Class Producing Graphs**"
      ],
      "metadata": {
        "id": "F-_gAkSoUod9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5kZMyIGJxZab"
      },
      "outputs": [],
      "source": [
        "class DataViz2:\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def scatter_plot(self, feature1, feature2):\n",
        "\n",
        "        # Separate the data into accepted and refused loans\n",
        "        accepted = self.data[self.data[target_column] == 1]\n",
        "        refused = self.data[self.data[target_column] == 0]\n",
        "        fig, ax = plt.subplots(figsize=(12, 10))\n",
        "\n",
        "        # Create the scatter plot with different colors for accepted and refused loans\n",
        "        ax.scatter(accepted[feature1], accepted[feature2], color='blue', alpha=0.9, label='Positive Evaluation')\n",
        "        ax.scatter(refused[feature1], refused[feature2], color='red', alpha=0.4, label='Negative Evaluation')\n",
        "\n",
        "        ax.set_xlabel(feature1)\n",
        "        ax.set_ylabel(feature2)\n",
        "\n",
        "        if l_dict:\n",
        "            if len(label_dict.get(feature1, []))>0:\n",
        "                lim = plt.gca().get_xlim()\n",
        "                xlim = tuple(int(x) for x in lim)\n",
        "                plt.xticks(list(range(xlim[0], xlim[1]+1)), label_dict[feature1][1][xlim[0]: xlim[1]+1], rotation=90)\n",
        "\n",
        "            if len(label_dict.get(feature2, []))>0:\n",
        "                lim = plt.gca().get_ylim()\n",
        "                ylim = tuple(int(y) for y in lim)\n",
        "                plt.yticks(list(range(ylim[0], ylim[1]+1)), label_dict[feature2][1][ylim[0]: ylim[1]+1])\n",
        "\n",
        "        ax.legend()\n",
        "\n",
        "        # Save the complete plot with axes and labels\n",
        "        fig.savefig('scatter.png')\n",
        "        plt.axis('off')\n",
        "        ax.legend().remove()\n",
        "        fig.savefig('scatter_both.png', bbox_inches='tight', pad_inches = 0)\n",
        "        plt.close()\n",
        "        # View the graph created\n",
        "        img = imageio.imread('scatter.png')\n",
        "        fig, ax = plt.subplots(figsize=(12, 10))\n",
        "        ax.imshow(img)\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "        # Create the scatter plot for accepted loans\n",
        "        fig_accepted, ax_accepted = plt.subplots(figsize=(12, 10))\n",
        "        ax_accepted.scatter(accepted[feature1], accepted[feature2], color='blue', alpha=1.0)\n",
        "        ax_accepted.axis('off')\n",
        "        fig_accepted.savefig( 'scatter_accepted.png', bbox_inches='tight', pad_inches = 0)\n",
        "        plt.close(fig_accepted)\n",
        "\n",
        "        # Create the scatter plot for refused loans\n",
        "        fig_rejected, ax_rejected = plt.subplots(figsize=(12, 10))\n",
        "        ax_rejected.scatter(refused[feature1], refused[feature2], color='red', alpha=1.0)\n",
        "        ax_rejected.axis('off')\n",
        "        fig_rejected.savefig('scatter_rejected.png', bbox_inches='tight', pad_inches = 0)\n",
        "        plt.close(fig_rejected)\n",
        "\n",
        "        image = imageio.imread('scatter_accepted.png')\n",
        "\n",
        "        # PROXIMITY\n",
        "\n",
        "        # Calculate the distance between each accepted point and its nearest accepted neighbor (normalized)\n",
        "        accepted_points = accepted[[feature1, feature2]].values\n",
        "        max_values = np.amax(accepted_points, axis=0)\n",
        "        min_values = np.amin(accepted_points, axis=0)\n",
        "        accepted_normalized = accepted_points / max_values\n",
        "        neighbors = NearestNeighbors(n_neighbors=2).fit(accepted_normalized)\n",
        "        distances, indices = neighbors.kneighbors(accepted_normalized)\n",
        "\n",
        "        # The first column of 'distances' is the distance to the point itself (always 0),\n",
        "        # so we take the second column\n",
        "        nearest_neighbor_distances = distances[:, 1]\n",
        "\n",
        "        # Compute the minimum, maximum, and average distance\n",
        "        min_distance = np.min(nearest_neighbor_distances)\n",
        "        max_distance = np.max(nearest_neighbor_distances)\n",
        "        average_distance = np.mean(nearest_neighbor_distances)\n",
        "\n",
        "        # Compute the distances in case the point are equidistributed within the graph\n",
        "        num_points = accepted_points.size\n",
        "        x_max, y_max = max_values[0], max_values[1]\n",
        "        x_min, y_min = min_values[0], min_values[1]\n",
        "        points_per_row = int(np.sqrt(num_points))\n",
        "        x_step = (x_max - x_min) / points_per_row\n",
        "        y_step = (y_max - y_min) / points_per_row\n",
        "        d = np.sqrt(x_step**2 + y_step**2)\n",
        "        d_max = np.sqrt((x_max)**2 + (y_max)**2)\n",
        "        d_norm = d / d_max\n",
        "\n",
        "        # Compute the proximity score\n",
        "        proximity = 1 - abs(average_distance - min_distance) / abs(d_norm - min_distance)\n",
        "\n",
        "\n",
        "        # CLOSURE\n",
        "        areas_blue = closure_2('scatter_accepted.png')\n",
        "        areas_red = closure_2('scatter_rejected.png')\n",
        "\n",
        "        # Compute the total area for each class in order to normalize\n",
        "        total_area_blue = sum(areas_blue)\n",
        "        total_area_red = sum(areas_red)\n",
        "\n",
        "        # Calculate the largest area for red and green separately\n",
        "        largest_area_red = max(areas_red)\n",
        "        largest_area_blue = max(areas_blue)\n",
        "\n",
        "        # Calculate standard deviation for red and green separately\n",
        "        std_dev_red = np.std(areas_red)/((max(areas_red)-min(areas_red))/2) if len(areas_red) > 1 else 1\n",
        "        std_dev_blue = np.std(areas_blue)/((max(areas_blue)-min(areas_blue))/2) if len(areas_blue) > 1 else 1\n",
        "\n",
        "        # Calculate closure score for red and green separately\n",
        "        closure_score_red = (largest_area_red / total_area_red) * std_dev_red\n",
        "        closure_score_blue = (largest_area_blue / total_area_blue) * std_dev_blue\n",
        "\n",
        "        closure_value = (closure_score_red + closure_score_blue)/2\n",
        "\n",
        "        #SYMMETRY\n",
        "        symmetry_score=calculate_symmetry('scatter_both.png')\n",
        "\n",
        "        # OVERLAPPING\n",
        "        overlap_score = calculate_overlap('scatter_accepted.png', 'scatter_rejected.png', 'white', 'white')\n",
        "\n",
        "        total_score = closure_value + proximity + symmetry_score + overlap_score\n",
        "\n",
        "        # Return a dictionary with the metrics\n",
        "\n",
        "        print({\n",
        "            'closure': closure_value,\n",
        "            'proximity': proximity,\n",
        "            'symmetry': symmetry_score,\n",
        "            'overlapping': overlap_score\n",
        "        })\n",
        "\n",
        "        return [proximity, closure_value, symmetry_score, overlap_score]\n",
        "\n",
        "\n",
        "    def histogram(self, feature1, feature2):\n",
        "\n",
        "        def compute_limit(df, feature, percentage):\n",
        "            # If feature takes on more values than the expected number of rectangles by setting perc\n",
        "            if df[feature].nunique() > 100/percentage:\n",
        "                min_val = df[feature].min()\n",
        "                max_val = df[feature].max()*1.000001\n",
        "                range_val = max_val - min_val\n",
        "                step = range_val * percentage / 100\n",
        "                min = np.min(df[feature]) // step * step\n",
        "                max = (np.max(df[feature]) + step - 1) // step * step + 1\n",
        "            else:\n",
        "                min = data[feature].min()\n",
        "                max = data[feature].max()\n",
        "                step = 0\n",
        "            return step, min, max\n",
        "\n",
        "        def compute_centers(verts):\n",
        "            centers=[]\n",
        "            for vert in verts:\n",
        "                l = ((vert.vertices[1][0] - vert.vertices[0][0]) / 2) + vert.vertices[0][0]\n",
        "                h = ((vert.vertices[2][1] - vert.vertices[0][1]) / 2) + vert.vertices[0][1]\n",
        "                centers.append([l, h])\n",
        "            return centers\n",
        "\n",
        "        # Complete graph\n",
        "\n",
        "        data = self.data[[feature1, feature2, target_column]].copy()\n",
        "        data[target_column] = data[target_column].replace(0, -1)\n",
        "        target_max = data[target_column][data[target_column] > 0].sum()\n",
        "        target_min = data[target_column][data[target_column] < 0].sum()\n",
        "\n",
        "        step1, min_feature1, max_feature1 = compute_limit(data, feature1, 10)\n",
        "        step2, min_feature2, max_feature2 = compute_limit(data, feature2, 10)\n",
        "\n",
        "        # Bin 'feature1' and 'feature2' into categories\n",
        "        if step1 > 0:\n",
        "            data[feature1] = pd.cut(data[feature1], bins=np.arange(min_feature1, max_feature1, step1), right=False)\n",
        "        else:\n",
        "            data[feature1] = pd.cut(data[feature1], bins=np.arange(min_feature1, max_feature1 + 2, 1), right=False)\n",
        "\n",
        "        if step2 > 0:\n",
        "            data[feature2] = pd.cut(data[feature2], bins=np.arange(min_feature2, max_feature2, step2), right=False)\n",
        "        else:\n",
        "            data[feature2] = pd.cut(data[feature2], bins=np.arange(min_feature2, max_feature2 + 2, 1), right=False)\n",
        "\n",
        "        feature1_categories = data[feature1].cat.categories\n",
        "        feature2_categories = data[feature2].cat.categories\n",
        "\n",
        "        hist_accepted = None\n",
        "        hist_rejected = None\n",
        "\n",
        "        for loan_approval, color_map, alpha in [(1, 'Blues', 0.9), (0, 'Reds', 0.5)]:\n",
        "            subset = self.data[self.data[target_column] == loan_approval]\n",
        "            subset[feature1] = np.sort(subset[feature1])\n",
        "            subset[feature2] = np.sort(subset[feature2])\n",
        "\n",
        "            # Bin 'feature1' and 'feature2' into categories\n",
        "            subset[feature1] = pd.cut(subset[feature1], bins=feature1_categories, right=False)\n",
        "            subset[feature2] = pd.cut(subset[feature2], bins=feature2_categories, right=False)\n",
        "\n",
        "            xy_subset = np.column_stack((subset[feature1].cat.codes, subset[feature2].cat.codes))\n",
        "            hist_subset = [len(xy_subset[(xy_subset == np.array([x, y])).all(axis=1)])for y in range(len(feature2_categories)-1, -1, -1) for x in range(len(feature1_categories))]\n",
        "            hist_subset = np.array(hist_subset).reshape(len(feature2_categories), len(feature1_categories))\n",
        "\n",
        "            if loan_approval == 1:\n",
        "                hist_accepted = hist_subset\n",
        "            else:\n",
        "                hist_rejected = hist_subset\n",
        "\n",
        "        # Create a plot for both accepted and rejected loans\n",
        "        fig_both, ax_both = plt.subplots(figsize=(12, 10))\n",
        "        cmap_accepted = plt.get_cmap('Blues')\n",
        "        cmap_rejected = plt.get_cmap('Reds')\n",
        "\n",
        "        ax_both.pcolormesh(hist_accepted, cmap=cmap_accepted, edgecolors='white', linewidths=2)\n",
        "        ax_both.pcolormesh(hist_rejected, cmap=cmap_rejected, alpha=0.5, edgecolors='white', linewidths=2)\n",
        "        ax_both.set_xticks(np.arange(len(feature1_categories)))\n",
        "        ax_both.set_xticklabels([str(category) for category in feature1_categories], rotation=45)\n",
        "        ax_both.set_yticks(np.arange(len(feature2_categories)))\n",
        "        ax_both.set_yticklabels([str(category) for category in feature2_categories])\n",
        "        ax_both.set_xlabel(feature1)\n",
        "        ax_both.set_ylabel(feature2)\n",
        "\n",
        "        if l_dict:\n",
        "            if len(label_dict.get(feature1, []))>0:\n",
        "                ax_both.set_xlim(0, max_feature1 + 0.9)\n",
        "                lim = plt.gca().get_xlim()\n",
        "                xlim = tuple(int(x) for x in lim)\n",
        "                plt.xticks([x + 0.5 for x in list(range(xlim[0], xlim[1]+1))], label_dict[feature1][1][xlim[0]: xlim[1]+1], rotate=45)\n",
        "\n",
        "            if len(label_dict.get(feature2, []))>0:\n",
        "                ax_both.set_ylim(0, max_feature2 + 0.9)\n",
        "                lim = plt.gca().get_ylim()\n",
        "                ylim = tuple(int(y) for y in lim)\n",
        "                plt.yticks([y + 0.5 for y in list(range(ylim[0], ylim[1]+1))], label_dict[feature2][1][ylim[0]: ylim[1]+1])\n",
        "\n",
        "        plt.savefig('histogram.png')\n",
        "        plt.axis('off')\n",
        "        plt.savefig('histogram_both.png', bbox_inches='tight', pad_inches = 0)\n",
        "        plt.close()\n",
        "        # View the graph created\n",
        "        img = imageio.imread('histogram.png')\n",
        "        fig, ax = plt.subplots(figsize=(12, 10))\n",
        "        ax.imshow(img)\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "        # Create a plot for only accepted loans\n",
        "        fig_accepted, ax_accepted = plt.subplots(figsize=(12, 10))\n",
        "        cmap = plt.get_cmap('Blues')\n",
        "        ax1 = ax_accepted.pcolormesh(hist_accepted, cmap=cmap, edgecolors='white', linewidths=2)\n",
        "        ax_accepted.set_xticks(np.arange(len(feature1_categories)))\n",
        "        ax_accepted.set_xticklabels([str(category) for category in feature1_categories])\n",
        "        ax_accepted.set_yticks(np.arange(len(feature2_categories)))\n",
        "        ax_accepted.set_yticklabels([str(category) for category in feature2_categories])\n",
        "        ax_accepted.set_xlabel(feature1)\n",
        "        ax_accepted.set_ylabel(feature2)\n",
        "        plt.axis('off')\n",
        "        plt.savefig('histogram_accepted.png', bbox_inches='tight', pad_inches = 0)\n",
        "\n",
        "        # Retrieves the vertices of rectangles\n",
        "        list_paths = ax1.get_paths()\n",
        "        # Calculate the coordinates of the centres\n",
        "        hist_centers = compute_centers(list_paths)\n",
        "        # Access to the Axes object associated with the pcolormesh graph\n",
        "        axes = plt.gca()\n",
        "        # Conversion of graph co-ordinates to screen co-ordinates\n",
        "        screen_point = axes.transData.transform(hist_centers)\n",
        "        # Calculate the maximum distance between the rectangles\n",
        "        max_his_dist = math.sqrt((screen_point[-1, 0] - screen_point[0, 0])**2 +\n",
        "                                 (screen_point[-1, 1] - screen_point[0, 1])**2)\n",
        "\n",
        "        # Retrieves frequencies associated with rectangles\n",
        "        frequencies = ax1.get_array()\n",
        "\n",
        "        # Calculate how many unique frequnces there are\n",
        "        frequencies_unique = np.unique(frequencies)\n",
        "\n",
        "        # CLOUSURE POISTIVES\n",
        "        areas_array, max_area = closure_1(ax1, frequencies, frequencies_unique)\n",
        "        plt.close('all')\n",
        "\n",
        "        total_area = sum(areas_array)\n",
        "        # Calculate the largest area for accepted and rejected separately\n",
        "        largest_area = max(areas_array)\n",
        "\n",
        "        # Calculate standard deviation for blu and red separately\n",
        "        std_dev = np.std(areas_array)/((max(areas_array)-min(areas_array))/2) if len(areas_array) > 1 else 1\n",
        "\n",
        "        # Calculate closure score for accepted and rejected separately\n",
        "        closure_score_positive = (largest_area / total_area) * std_dev\n",
        "\n",
        "        # PROXIMITY POSITIVES\n",
        "        dist = proximity_1(screen_point, frequencies, perc=5)\n",
        "\n",
        "        # Normalize distances with image diagonal\n",
        "        normalized_distances = np.array(dist) / max_his_dist\n",
        "        min_distance = np.min(normalized_distances)\n",
        "        max_distance = np.max(normalized_distances)\n",
        "\n",
        "        # Calculate mean of distances\n",
        "        average_distance = np.mean(normalized_distances)\n",
        "\n",
        "        # Calculate standard deviation of distances\n",
        "        std_distance = np.std(normalized_distances)\n",
        "\n",
        "        # Calculate percentiles of normalized distances\n",
        "        min_ratio = np.percentile(normalized_distances, 5)\n",
        "        max_ratio = np.percentile(normalized_distances, 95)\n",
        "\n",
        "        # Calculate percentages of points too close or too distant to their nearest neighbors\n",
        "        too_close = len(np.where(normalized_distances <= min_ratio)[0]) / len(dist)\n",
        "        too_distance = len(np.where(normalized_distances >= max_ratio)[0]) / len(dist)\n",
        "\n",
        "        # Calculate proximity score\n",
        "        proximity_accepted_score = (min_distance + max_distance + average_distance + (1 - too_close) + (1 - too_distance) +\n",
        "                                    std_distance) / 6\n",
        "\n",
        "        # Create a plot for only rejected loans\n",
        "        fig_rejected, ax_rejected = plt.subplots(figsize=(12, 10))\n",
        "        cmap = plt.get_cmap('Reds')\n",
        "        ax2 = ax_rejected.pcolormesh(hist_rejected, cmap=cmap, edgecolors='white', linewidths=2)\n",
        "        ax_rejected.set_xticks(np.arange(len(feature1_categories)))\n",
        "        ax_rejected.set_xticklabels([str(category) for category in feature1_categories])\n",
        "        ax_rejected.set_yticks(np.arange(len(feature2_categories)))\n",
        "        ax_rejected.set_yticklabels([str(category) for category in feature2_categories])\n",
        "        ax_rejected.set_xlabel(feature1)\n",
        "        ax_rejected.set_ylabel(feature2)\n",
        "        plt.axis('off')\n",
        "        plt.savefig('histogram_rejected.png', bbox_inches='tight', pad_inches = 0)\n",
        "\n",
        "        # Retrieves frequencies associated with rectangles\n",
        "        list_paths = ax2.get_paths()\n",
        "        # Calculate the coordinates of the centres\n",
        "        hist_centers = compute_centers(list_paths)\n",
        "        # Access to the Axes object associated with the pcolormesh graph\n",
        "        axes = plt.gca()\n",
        "        # Conversion of graph co-ordinates to screen co-ordinates\n",
        "        screen_point = axes.transData.transform(hist_centers)\n",
        "\n",
        "        # Retrieves frequencies associated with rectangles\n",
        "        frequencies = ax2.get_array()\n",
        "        # Calculate how many unique frequnces there are\n",
        "        frequencies_unique = np.unique(frequencies)\n",
        "\n",
        "        # CLOUSURE NEGATIVE\n",
        "        areas_array, max_area = closure_1(ax2, frequencies, frequencies_unique)\n",
        "        plt.close('all')\n",
        "\n",
        "        total_area = sum(areas_array)\n",
        "        # Calculate the largest area for accepted and rejected separately\n",
        "        largest_area = max(areas_array)\n",
        "\n",
        "        # Calculate standard deviation for blu and red separately\n",
        "        std_dev = np.std(areas_array)/((max(areas_array)-min(areas_array))/2) if len(areas_array) > 1 else 1\n",
        "\n",
        "        # Calculate closure score for accepted and rejected separately\n",
        "        closure_score_negative = (largest_area / total_area) * std_dev\n",
        "\n",
        "        closure_score = (closure_score_positive + closure_score_negative)/2\n",
        "\n",
        "        # PROXIMITY NEGATIVES\n",
        "        dist = proximity_1(screen_point, frequencies, perc=5)\n",
        "\n",
        "        # Normalize distances with image diagonal\n",
        "        normalized_distances = np.array(dist) / max_his_dist\n",
        "        min_distance = np.min(normalized_distances)\n",
        "        max_distance = np.max(normalized_distances)\n",
        "\n",
        "        # Calculate mean of distances\n",
        "        average_distance = np.mean(normalized_distances)\n",
        "\n",
        "        # Calculate standard deviation of distances\n",
        "        std_distance = np.std(normalized_distances)\n",
        "\n",
        "        # Calculate percentiles of normalized distances\n",
        "        min_ratio = np.percentile(normalized_distances, 5)\n",
        "        max_ratio = np.percentile(normalized_distances, 95)\n",
        "\n",
        "        # Calculate percentages of points too close or too distant to their nearest neighbors\n",
        "        too_close = len(np.where(normalized_distances <= min_ratio)[0]) / len(dist)\n",
        "        too_distance = len(np.where(normalized_distances >= max_ratio)[0]) / len(dist)\n",
        "\n",
        "        # Calculate proximity score\n",
        "        proximity_rejected_score = (min_distance + max_distance + average_distance + (1 - too_close) + (1 - too_distance) +\n",
        "                                    std_distance) / 6\n",
        "\n",
        "        #print('proximity score', proximity_rejected_score)\n",
        "\n",
        "        proximity_score = (proximity_accepted_score + proximity_rejected_score) / 2\n",
        "\n",
        "        # SYMMETRY\n",
        "        symmetry_score = calculate_symmetry('histogram_both.png')\n",
        "\n",
        "        # OVERLAPPING\n",
        "        overlap_score = calculate_overlap('histogram_accepted.png', 'histogram_rejected.png', 'blue', 'red')\n",
        "\n",
        "\n",
        "        total_score=closure_score+proximity_score+symmetry_score+overlap_score\n",
        "\n",
        "        print({\n",
        "            'closure': closure_score,\n",
        "            'proximity': proximity_score,\n",
        "            'symmetry': symmetry_score,\n",
        "            'overlap' : overlap_score\n",
        "        })\n",
        "\n",
        "        return [proximity_score, closure_score, symmetry_score, overlap_score]\n",
        "\n",
        "\n",
        "\n",
        "    def heatmap(self, feature1, feature2):\n",
        "\n",
        "        # Compute the graph for the complete heatmap, the heatmap without axis, the heatmap of the only accepted points, and the heatmap of only the rejected points\n",
        "        data = self.data[[feature1, feature2, target_column]].copy()\n",
        "        data[target_column] = data[target_column].replace(0, -1)\n",
        "        target_max = data[target_column][data[target_column] > 0].sum()\n",
        "        target_min = data[target_column][data[target_column] < 0].sum()\n",
        "\n",
        "        def compute_limit(df, feature, percentage):\n",
        "            if df[feature].nunique() > 100/percentage:\n",
        "                min_val = df[feature].min()\n",
        "                max_val = df[feature].max()*1.000001\n",
        "                range_val = max_val - min_val\n",
        "                step = range_val * percentage / 100\n",
        "                min = np.min(df[feature]) // step * step\n",
        "                max = (np.max(df[feature]) + step - 1) // step * step + 1\n",
        "            else:\n",
        "                min = data[feature].min()\n",
        "                max = data[feature].max()\n",
        "                step = 0\n",
        "            return step, min, max\n",
        "\n",
        "        step1, min_feature1, max_feature1 = compute_limit(data, feature1, 10)\n",
        "        step2, min_feature2, max_feature2 = compute_limit(data, feature2, 10)\n",
        "\n",
        "        # Bin 'feature1' and 'feature2' into categories\n",
        "        if step1 > 0:\n",
        "            data[feature1] = pd.cut(data[feature1], bins=np.arange(min_feature1, max_feature1, step1), right=False)\n",
        "        else:\n",
        "            data[feature1] = pd.cut(data[feature1], bins=np.arange(min_feature1, max_feature1 + 2, 1), right=False)\n",
        "\n",
        "        if step2 > 0:\n",
        "            data[feature2] = pd.cut(data[feature2], bins=np.arange(min_feature2, max_feature2, step2), right=False)\n",
        "        else:\n",
        "            data[feature2] = pd.cut(data[feature2], bins=np.arange(min_feature2, max_feature2 + 2, 1), right=False)\n",
        "\n",
        "        # Creating a pivot table with target sum (considering the sign)\n",
        "        pivot_table_all = pd.pivot_table(data, values=target_column, index=feature2, columns=feature1, aggfunc='sum')\n",
        "\n",
        "        # Calculate the absolute value while keeping the sign\n",
        "        pivot_table_norm = pivot_table_all.applymap(lambda x: x / target_max if x > 0 else -x / target_min)\n",
        "\n",
        "        # Create the complete heatmap\n",
        "        plt.figure(figsize=(12, 10))\n",
        "\n",
        "        # Creating a pivot table with absolute value\n",
        "        pivot_table_abs = pivot_table_norm.abs()\n",
        "\n",
        "        # Creating a heatmap with seaborn with absolute format in annotations\n",
        "        ax_complete = sns.heatmap(pivot_table_norm, annot=pivot_table_abs.applymap(lambda x: f\"{abs(x):.2f}\"), fmt=\"\", cmap='RdBu', linewidths=1.0, cbar_kws={'label': 'All Loans'}, center=0)\n",
        "\n",
        "        # Invert y-axis\n",
        "        ax_complete.invert_yaxis()\n",
        "\n",
        "        if l_dict:\n",
        "            if len(label_dict.get(feature1, []))>0:\n",
        "                ax_complete.set_xlim(0, max_feature1 + 0.9)\n",
        "                lim = plt.gca().get_xlim()\n",
        "                xlim = tuple(int(x) for x in lim)\n",
        "                plt.xticks([x + 0.5 for x in list(range(xlim[0], xlim[1]+1))], label_dict[feature1][1][xlim[0]: xlim[1]+1], rotate=90)\n",
        "\n",
        "            if len(label_dict.get(feature2, []))>0:\n",
        "                ax_complete.set_ylim(0, max_feature2 + 0.9)\n",
        "                lim = plt.gca().get_ylim()\n",
        "                ylim = tuple(int(y) for y in lim)\n",
        "                plt.yticks([y + 0.5 for y in list(range(ylim[0], ylim[1]+1))], label_dict[feature2][1][ylim[0]: ylim[1]+1])\n",
        "\n",
        "        # Save the complete heatmap\n",
        "        ax_complete.figure.savefig('heatmap.png')\n",
        "        # Removing the colour bar\n",
        "        ax_complete.collections[0].colorbar.remove()\n",
        "        # Hiding annotations\n",
        "        for text in ax_complete.texts:\n",
        "            text.set_visible(False)\n",
        "        plt.axis('off')\n",
        "        ax_complete.figure.savefig('heatmap_both.png', bbox_inches='tight', pad_inches=0)\n",
        "        plt.close()\n",
        "        # View the graph created\n",
        "        img = imageio.imread('heatmap.png')\n",
        "        fig, ax = plt.subplots(figsize=(12, 10))\n",
        "        ax.imshow(img)\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "        num_rows, num_cols = pivot_table_all.shape\n",
        "\n",
        "        # Calculate the centre points of the bars\n",
        "        center_x = range(num_cols)\n",
        "        center_y = range(num_rows)\n",
        "        centroids_all = []\n",
        "\n",
        "        for x in center_x:\n",
        "            for y in center_y:\n",
        "                center_x_data = x + 0.5\n",
        "                center_y_data = y + 0.5\n",
        "                center_x_real, center_y_real = ax_complete.transData.transform([center_x_data, center_y_data])\n",
        "                centroids_all.append((center_x_real, center_y_real))\n",
        "        # Calculate the maximum distance\n",
        "        image_diagonal = math.sqrt((centroids_all[0][0]-centroids_all[num_cols*num_rows-1][0])**2+(centroids_all[0][1]-centroids_all[num_cols*num_rows-1][1])**2)\n",
        "\n",
        "        # Create the accepted heatmap\n",
        "        data[target_column] = data[target_column].replace(-1, 0)\n",
        "\n",
        "        # Creating a pivot table with target sum (considering the sign)\n",
        "        pivot_table_positive = pd.pivot_table(data, values=target_column, index=feature2, columns=feature1, aggfunc='sum')\n",
        "\n",
        "        # Calculate the absolute value while keeping the sign\n",
        "        pivot_table_norm = pivot_table_positive.applymap(lambda x: x / target_max if x > 0 else -x / target_min)\n",
        "\n",
        "        # Heatmap dimensions\n",
        "        plt.figure(figsize=(12, 10))\n",
        "\n",
        "        # Create heatmap with seaborn\n",
        "        ax_positive = sns.heatmap(pivot_table_norm, cmap='RdBu', linewidths=1.0, cbar=None, center=0)\n",
        "\n",
        "        # Invert y-axis\n",
        "        ax_positive.invert_yaxis()\n",
        "        # Show the plot without axes and legend\n",
        "        plt.axis('off')\n",
        "\n",
        "        for x in center_x:\n",
        "            for y in center_y:\n",
        "                center_x_data = x + 0.5\n",
        "                center_y_data = y + 0.5\n",
        "                center_x_real, center_y_real = ax_positive.transData.transform([center_x_data, center_y_data])\n",
        "                centroids_all.append((center_x_real, center_y_real))\n",
        "\n",
        "        # Save the heatmap for accepted loans\n",
        "        ax_positive.figure.savefig('heatmap_positive.png', bbox_inches='tight', pad_inches=0)\n",
        "        frequencies = np.round([ax_positive.collections[0].get_array() * 100])\n",
        "        frequencies = frequencies.astype(int).reshape(-1)\n",
        "        frequencies_unique = np.unique(frequencies)\n",
        "        plt.close()\n",
        "\n",
        "        # PROXIMITY POSITIVES\n",
        "        dist = proximity_1(np.array(centroids_all), frequencies, perc=5)\n",
        "\n",
        "        # Normalize distances with image diagonal\n",
        "        normalized_distances = np.array(dist) / image_diagonal\n",
        "        min_distance = np.min(normalized_distances)\n",
        "        max_distance = np.max(normalized_distances)\n",
        "\n",
        "        # Calculate mean of distances\n",
        "        average_distance = np.mean(normalized_distances)\n",
        "\n",
        "        # Calculate standard deviation of distances\n",
        "        std_distance = np.std(normalized_distances)\n",
        "\n",
        "        # Calculate percentiles of normalized distances\n",
        "        min_ratio = np.percentile(normalized_distances, 5)\n",
        "        max_ratio = np.percentile(normalized_distances, 95)\n",
        "\n",
        "        # Calculate percentages of points too close or too distant to their nearest neighbors\n",
        "        too_close = len(np.where(normalized_distances <= min_ratio)[0]) / len(dist)\n",
        "        too_distance = len(np.where(normalized_distances >= max_ratio)[0]) / len(dist)\n",
        "\n",
        "        # Calculate proximity score\n",
        "        proximity_positive_score = (min_distance + max_distance + average_distance + (1 - too_close) + (1 - too_distance) +\n",
        "                                    std_distance) / 6\n",
        "\n",
        "        # CLOUSURE POISTIVES\n",
        "        mat = np.zeros((num_rows, num_cols))\n",
        "        _, h2d = plt.subplots(figsize=(12, 10))\n",
        "        # Create a pcolormesh with the same dimension to use closure_1 function\n",
        "        h2d = plt.pcolormesh(mat , cmap='Blues', vmin=min(frequencies), vmax=max(frequencies), edgecolors='white', linewidths=2)\n",
        "        plt.axis('off')\n",
        "\n",
        "        areas_array, max_area = closure_1(h2d, frequencies, frequencies_unique)\n",
        "        plt.close('all')\n",
        "\n",
        "        total_area = sum(areas_array)\n",
        "        # Calculate the largest area for accepted and rejected separately\n",
        "        largest_area = max(areas_array)\n",
        "\n",
        "        # Calculate standard deviation for blu and red separately\n",
        "        std_dev = np.std(areas_array)/((max(areas_array)-min(areas_array))/2) if len(areas_array) > 1 else 1\n",
        "\n",
        "        # Calculate closure score for accepted and rejected separately\n",
        "        closure_score_positive = (largest_area / total_area) * std_dev\n",
        "\n",
        "        # Create the rejected heatmap\n",
        "        data[target_column] = [x-1 for x in data[target_column]]\n",
        "\n",
        "        # Creating a pivot table with target sum (considering the sign)\n",
        "        pivot_table_negative = pd.pivot_table(data, values=target_column, index=feature2, columns=feature1, aggfunc='sum')\n",
        "\n",
        "        # Calculate the absolute value while keeping the sign\n",
        "        pivot_table_norm = pivot_table_negative.applymap(lambda x: x / target_max if x > 0 else -x / target_min)\n",
        "\n",
        "        pivot_table_abs = pivot_table_norm.abs()\n",
        "\n",
        "        # Create the complete heatmap\n",
        "        plt.figure(figsize=(12, 10))\n",
        "\n",
        "        # Create heatmap with seaborn\n",
        "        ax_negative = sns.heatmap(pivot_table_norm, cmap='RdBu', linewidths=1.0, cbar=None, center=0)\n",
        "\n",
        "        # Invert y-axis\n",
        "        ax_negative.invert_yaxis()\n",
        "\n",
        "        for x in center_x:\n",
        "            for y in center_y:\n",
        "                center_x_data = x + 0.5\n",
        "                center_y_data = y + 0.5\n",
        "                center_x_real, center_y_real = ax_negative.transData.transform([center_x_data, center_y_data])\n",
        "                centroids_all.append((center_x_real, center_y_real))\n",
        "\n",
        "        # Show the plot without axes and legend\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Save the heatmap for accepted loans\n",
        "        ax_negative.figure.savefig('heatmap_negative.png', bbox_inches='tight', pad_inches=0)\n",
        "        frequencies = np.abs(np.round([ax_negative.collections[0].get_array() * 100]))\n",
        "        frequencies = frequencies.astype(int).reshape(-1)\n",
        "        frequencies_unique = np.unique(frequencies)\n",
        "        plt.close()\n",
        "\n",
        "        mat = np.ones((num_rows, num_cols))\n",
        "        _, h2d = plt.subplots(figsize=(12, 10))\n",
        "        # Create a pcolormesh with the same dimension to use closure_1 function\n",
        "        h2d = plt.pcolormesh(mat , cmap='Blues', vmin=min(frequencies), vmax=max(frequencies), edgecolors='white', linewidths=2)\n",
        "        plt.axis('off')\n",
        "\n",
        "        # PROXIMITY NEGATIVES\n",
        "        dist = proximity_1(np.array(centroids_all), frequencies, perc=5)\n",
        "\n",
        "        # Normalize distances with image diagonal\n",
        "        normalized_distances = np.array(dist) / image_diagonal\n",
        "        min_distance = np.min(normalized_distances)\n",
        "        max_distance = np.max(normalized_distances)\n",
        "\n",
        "        # Calculate mean of distances\n",
        "        average_distance = np.mean(normalized_distances)\n",
        "\n",
        "        # Calculate standard deviation of distances\n",
        "        std_distance = np.std(normalized_distances)\n",
        "\n",
        "        # Calculate percentiles of normalized distances\n",
        "        min_ratio = np.percentile(normalized_distances, 5)\n",
        "        max_ratio = np.percentile(normalized_distances, 95)\n",
        "\n",
        "        # Calculate percentages of points too close or too distant to their nearest neighbors\n",
        "        too_close = len(np.where(normalized_distances <= min_ratio)[0]) / len(dist)\n",
        "        too_distance = len(np.where(normalized_distances >= max_ratio)[0]) / len(dist)\n",
        "\n",
        "        # Calculate proximity score\n",
        "        proximity_negative_score = (min_distance + max_distance + average_distance + (1 - too_close) + (1 - too_distance) +\n",
        "                                    std_distance) / 6\n",
        "\n",
        "        # CLOUSURE NEGATIVES\n",
        "        areas_array, max_area = closure_1(h2d, frequencies, frequencies_unique)\n",
        "        plt.close('all')\n",
        "\n",
        "        total_area = sum(areas_array)\n",
        "        # Calculate the largest area for accepted and rejected separately\n",
        "        largest_area = max(areas_array)\n",
        "\n",
        "        # Calculate standard deviation for blu and red separately\n",
        "        std_dev = np.std(areas_array)/((max(areas_array)-min(areas_array))/2) if len(areas_array) > 1 else 1\n",
        "\n",
        "        # Calculate closure score for accepted and rejected separately\n",
        "        closure_score_negative = (largest_area / total_area) * std_dev\n",
        "\n",
        "        closure_score = (closure_score_positive + closure_score_negative)/2\n",
        "\n",
        "        proximity_score = (proximity_positive_score + proximity_negative_score) / 2\n",
        "\n",
        "        # SYMMETRY\n",
        "        symmetry_score = calculate_symmetry('heatmap_both.png')\n",
        "\n",
        "        # OVERLAPPING\n",
        "        overlap_score = calculate_overlap('heatmap_positive.png', 'heatmap_negative.png', 'white', 'white')\n",
        "\n",
        "        # Compute the metrics for the graph\n",
        "        closure_value = closure_score\n",
        "        proximity_value = proximity_score\n",
        "        symmetry_value = symmetry_score\n",
        "\n",
        "        total_score=closure_score+proximity_score+symmetry_score+overlap_score\n",
        "\n",
        "        print({\n",
        "            'closure': closure_value,\n",
        "            'proximity': proximity_value,\n",
        "            'symmetry': symmetry_value,\n",
        "            'overlap' : overlap_score\n",
        "        })\n",
        "\n",
        "\n",
        "        return [proximity_score, closure_value, symmetry_score, overlap_score]\n",
        "\n",
        "\n",
        "    def hexbin_plot(self, category1, category2):\n",
        "\n",
        "        accepted = self.data[self.data[target_column] == 1]\n",
        "\n",
        "        rejected = self.data[self.data[target_column] == 0]\n",
        "\n",
        "\n",
        "        # Calculate the same x- and y-axis limits for both accepted and rejected graphs\n",
        "        x_min = min(accepted[category1].min(), rejected[category1].min())\n",
        "        x_max = max(accepted[category1].max(), rejected[category1].max())\n",
        "        y_min = min(accepted[category2].min(), rejected[category2].min())\n",
        "        y_max = max(accepted[category2].max(), rejected[category2].max())\n",
        "\n",
        "\n",
        "        # Set gridsize dimension\n",
        "        gridsize = 10\n",
        "\n",
        "        # Plot the complete graph\n",
        "        _, ax = plt.subplots(figsize=(12, 10))\n",
        "        plt.tight_layout()\n",
        "        extent = [x_min, x_max, y_min, y_max]\n",
        "        _ = ax.hexbin(accepted[category1], accepted[category2], gridsize=gridsize, cmap='Blues', alpha=0.5, extent=extent )\n",
        "        _ = ax.hexbin(rejected[category1], rejected[category2], gridsize=gridsize, cmap='Reds', alpha=0.5, extent=extent)\n",
        "\n",
        "        # Add axis labels\n",
        "        ax.set_xlabel(category1)\n",
        "        ax.set_ylabel(category2)\n",
        "\n",
        "        if l_dict:\n",
        "            if len(label_dict.get(category1, []))>0:\n",
        "                ax.set_xlim(0, self.data[category1].max() + 0.9)\n",
        "                lim = plt.gca().get_xlim()\n",
        "                xlim = tuple(int(x) for x in lim)\n",
        "                plt.xticks([x + 0.5 for x in list(range(xlim[0], xlim[1]+1))], label_dict[feature1][1][xlim[0]: xlim[1]+1], rotate=45)\n",
        "\n",
        "            if len(label_dict.get(category2, []))>0:\n",
        "                lim = plt.gca().get_ylim()\n",
        "                ylim = tuple(int(y) for y in lim)\n",
        "                plt.yticks(label_dict[feature2][0][:], label_dict[feature2][1][:])\n",
        "\n",
        "        plt.savefig('hexbin.png')\n",
        "        plt.axis('off')\n",
        "        plt.savefig('hexbin_both.png', bbox_inches='tight', pad_inches=0)\n",
        "        plt.close()\n",
        "\n",
        "        # View the graph created\n",
        "        img = imageio.imread('hexbin.png')\n",
        "        fig, ax = plt.subplots(figsize=(12, 10))\n",
        "        ax.imshow(img)\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "        plt.figure(figsize=(12, 10))\n",
        "        cmap = plt.cm.Blues\n",
        "        _, ax1 = plt.subplots(figsize=(12, 10))\n",
        "\n",
        "        # Set the range of variation of the variables on the same axes for all graphs in order to compare them\n",
        "        extent = [x_min, x_max, y_min, y_max]\n",
        "\n",
        "        # Create the graph for accepted\n",
        "        hb1 = ax1.hexbin(accepted[category1], accepted[category2], gridsize=gridsize, cmap=cmap, edgecolors='white', extent=extent)\n",
        "\n",
        "        # Calculate the coordinates of the centres\n",
        "        verts_accepted = hb1.get_offsets()\n",
        "\n",
        "        # Conversion of graph co-ordinates to screen co-ordinates\n",
        "        verts_screen_accepted = hb1.get_transform().transform(verts_accepted)\n",
        "\n",
        "        # Calculate the distance between the centre of the first hexagon and the last hexagon to derive the maximum distance for hexbins\n",
        "        # Find the index of the topmost hexagon to the right\n",
        "        index_max_col1 = np.where(verts_screen_accepted[:, 0] == verts_screen_accepted[:, 0].max())[0]\n",
        "        index_max_col2 = np.where(verts_screen_accepted[:, 1] == verts_screen_accepted[:, 1].max())[0]\n",
        "\n",
        "        # Find the common index between index_max_col1 and index_max_col2\n",
        "        common_index = np.intersect1d(index_max_col1, index_max_col2)[0]\n",
        "        # Calculate the maximum distance\n",
        "        max_hex_dist = math.sqrt((verts_screen_accepted[common_index, 0] - verts_screen_accepted[0, 0])**2 +\n",
        "                                 (verts_screen_accepted[common_index, 1] - verts_screen_accepted[0, 1])**2)\n",
        "\n",
        "        # Retrieves frequencies associated with hexagons\n",
        "        freq_accepted = hb1.get_array()\n",
        "        # For each frequency calculate the value of the associated color shade\n",
        "        color_accepted = cmap(freq_accepted/np.max(freq_accepted))\n",
        "        plt.axis('off')\n",
        "        plt.savefig('hexbin_accepted.png', bbox_inches='tight', pad_inches=0)\n",
        "\n",
        "        frequencies_unique = np.unique(freq_accepted)\n",
        "\n",
        "        # CLOSURE POSITIVES\n",
        "        areas_accepted, max_area = closure_1(hb1, freq_accepted, frequencies_unique)\n",
        "        plt.close('all')\n",
        "\n",
        "        # Calculate the largest area for accepted and rejected separately\n",
        "        largest_area_accepted = max(areas_accepted)\n",
        "\n",
        "        # Calculate standard deviation for blu and red separately\n",
        "        std_dev_accepted = np.std(areas_accepted)/((max(areas_accepted)-min(areas_accepted))/2) if len(areas_accepted) > 1 else 1\n",
        "\n",
        "        # Calculate closure score for accepted\n",
        "        closure_score_accepted = (largest_area_accepted / max_area) * std_dev_accepted\n",
        "\n",
        "        # PROXIMITY POSITIVE\n",
        "        # Calculate proximity for accepted\n",
        "        dist = proximity_1(verts_screen_accepted, freq_accepted, perc=5)\n",
        "\n",
        "        # Normalize distances with image diagonal\n",
        "        normalized_distances = np.array(dist) / max_hex_dist\n",
        "\n",
        "        min_distance = np.min(normalized_distances)\n",
        "        max_distance = np.max(normalized_distances)\n",
        "\n",
        "        # Calculate mean of distances\n",
        "        average_distance = np.mean(normalized_distances)\n",
        "\n",
        "        # Calculate standard deviation of distances\n",
        "        std_distance = np.std(normalized_distances)\n",
        "\n",
        "        # Calculate percentiles of normalized distances\n",
        "        min_ratio = np.percentile(normalized_distances, 5)\n",
        "        max_ratio = np.percentile(normalized_distances, 95)\n",
        "\n",
        "        # Calculate percentages of points too close or too distant to their nearest neighbors\n",
        "        too_close = len(np.where(normalized_distances <= min_ratio)[0]) / len(dist)\n",
        "        too_distance = len(np.where(normalized_distances >= max_ratio)[0]) / len(dist)\n",
        "\n",
        "        # Calculate proximity score\n",
        "        proximity_accepted_score = (min_distance + max_distance + average_distance + (1 - too_close) + (1 - too_distance) +\n",
        "                                    std_distance) / 6\n",
        "\n",
        "        plt.figure(figsize=(12, 10))\n",
        "        cmap = plt.cm.Reds\n",
        "        _, ax2 = plt.subplots(figsize=(12, 10))\n",
        "        hb2 = ax2.hexbin(rejected[category1], rejected[category2], gridsize=gridsize, cmap=cmap, edgecolors='white', extent=extent)\n",
        "        verts_rejected = hb2.get_offsets()\n",
        "        verts_screen_rejected = hb2.get_transform().transform(verts_rejected)\n",
        "        freq_rejected = hb2.get_array()\n",
        "        color_rejected = cmap(freq_rejected/np.max(freq_rejected))\n",
        "        plt.axis('off')\n",
        "        plt.savefig('hexbin_rejected.png', bbox_inches='tight', pad_inches=0)\n",
        "\n",
        "        # CLOSURE NEGATIVES\n",
        "\n",
        "        frequencies_unique = np.unique(freq_rejected)\n",
        "\n",
        "        # Calculate areas for rejected\n",
        "        areas_rejected, max_area = closure_1(hb2, freq_rejected, frequencies_unique)\n",
        "        plt.close('all')\n",
        "\n",
        "        # Calculate the largest area for rejected\n",
        "        largest_area_rejected = max(areas_rejected)\n",
        "\n",
        "        # Calculate standard deviation\n",
        "        std_dev_rejected = np.std(areas_rejected)/((max(areas_rejected)-min(areas_rejected))/2) if len(areas_rejected) > 1 else 1\n",
        "\n",
        "        # Calculate closure score for accepted\n",
        "        closure_score_rejected = (largest_area_rejected / max_area) * std_dev_rejected\n",
        "\n",
        "        closure_score = (closure_score_accepted + closure_score_rejected)/2\n",
        "\n",
        "        # PROXMITY NEGATIVES\n",
        "        dist = proximity_1(verts_screen_rejected, freq_rejected, perc=5)\n",
        "\n",
        "        # Normalize distances with image diagonal\n",
        "        normalized_distances = np.array(dist) / max_hex_dist\n",
        "\n",
        "        min_distance = np.min(normalized_distances)\n",
        "        max_distance = np.max(normalized_distances)\n",
        "\n",
        "        # Calculate mean of distances\n",
        "        average_distance = np.mean(normalized_distances)\n",
        "\n",
        "        # Calculate standard deviation of distances\n",
        "        std_distance = np.std(normalized_distances)\n",
        "\n",
        "        # Calculate percentiles of normalized distances\n",
        "        min_ratio = np.percentile(normalized_distances, 5)\n",
        "        max_ratio = np.percentile(normalized_distances, 95)\n",
        "\n",
        "        # Calculate percentages of points too close or too distant to their nearest neighbors\n",
        "        too_close = len(np.where(normalized_distances <= min_ratio)[0]) / len(dist)\n",
        "        too_distance = len(np.where(normalized_distances >= max_ratio)[0]) / len(dist)\n",
        "\n",
        "        # Calculate proximity score\n",
        "        proximity_rejected_score = (min_distance + max_distance + average_distance + (1 - too_close) + (1 - too_distance) +\n",
        "                                    std_distance) / 6\n",
        "\n",
        "        proximity_score = (proximity_accepted_score + proximity_rejected_score) / 2\n",
        "\n",
        "        # SYMMETRY\n",
        "        symmetry_score = calculate_symmetry('hexbin_both.png')\n",
        "\n",
        "        # OVERLAPPING\n",
        "        overlap_score = calculate_overlap('hexbin_accepted.png', 'hexbin_rejected.png', 'blue', 'red')\n",
        "\n",
        "        # Calculate metrics for the plot\n",
        "        closure_value = closure_score\n",
        "        proximity_value = proximity_score\n",
        "        symmetry_value = symmetry_score\n",
        "\n",
        "        total_score=closure_value+proximity_value+symmetry_value+overlap_score\n",
        "\n",
        "        print({\n",
        "            'closure': closure_value,\n",
        "            'proximity': proximity_value,\n",
        "            'symmetry': symmetry_value,\n",
        "            'overlap': overlap_score\n",
        "        })\n",
        "\n",
        "        return [proximity_value, closure_value, symmetry_value, overlap_score]\n",
        "\n",
        "\n",
        "    def kde_plot(self, category1, category2):\n",
        "\n",
        "        # Create a subset of the DataFrame for accepted loans\n",
        "        accepted = self.data[self.data[target_column] == 1].copy()\n",
        "\n",
        "        # Create a subset of the DataFrame for refused loans\n",
        "        refused = self.data[self.data[target_column] == 0].copy()\n",
        "\n",
        "        # Combine the data from both categories\n",
        "        combined = pd.concat([accepted, refused])\n",
        "\n",
        "        # Create the 2D Kernel Density Plot\n",
        "        plt.tight_layout()\n",
        "        plt.figure(figsize=(12, 10))\n",
        "\n",
        "        # Plot KDE for accepted loans\n",
        "        _, ax1 = plt.subplots(figsize=(12, 10))\n",
        "        ax1 = sns.kdeplot(data=accepted, x=category1, y=category2, fill=True, cmap='Blues', alpha=1.0, linewidths=1.0, edgecolor='white', label='Positive')\n",
        "        plt.xlabel(category1)\n",
        "        plt.ylabel(category2)\n",
        "        plt.axis('off')\n",
        "        plt.savefig('kde_accepted.png', bbox_inches='tight', pad_inches=0)\n",
        "        plt.close()\n",
        "\n",
        "        # Plot KDE for refused loans\n",
        "        _, ax2 = plt.subplots(figsize=(12, 10))\n",
        "        ax2 = sns.kdeplot(data=refused, x=category1, y=category2, fill=True, cmap='Reds', alpha=1.0, linewidths=1.0, edgecolor='white', label='Refused')\n",
        "        plt.xlabel(category1)\n",
        "        plt.ylabel(category2)\n",
        "        plt.axis('off')\n",
        "        plt.savefig('kde_refused.png', bbox_inches='tight', pad_inches=0)\n",
        "        plt.close()\n",
        "\n",
        "\n",
        "        # Complete graph\n",
        "        _ = sns.kdeplot(data=accepted, x=category1, y=category2, fill=True, cmap='Blues', alpha=0.5, linewidths=1.0, edgecolor='White', label='Positive')\n",
        "\n",
        "        # Plot KDE for refused loans\n",
        "        _ = sns.kdeplot(data=refused, x=category1, y=category2, fill=True, cmap='Reds', alpha=0.5, linewidths=1.0, edgecolor='White', label='Negative')\n",
        "\n",
        "        plt.xlabel(category1)\n",
        "        plt.ylabel(category2)\n",
        "\n",
        "        if l_dict:\n",
        "            if len(label_dict.get(category1, []))>0:\n",
        "                lim = plt.gca().get_xlim()\n",
        "                xlim = tuple(int(x) for x in lim)\n",
        "                plt.xticks(list(range(xlim[0], xlim[1]+1)), label_dict[category1][1][xlim[0]: xlim[1]+1], rotation=90)\n",
        "\n",
        "            if len(label_dict.get(category2, []))>0:\n",
        "                lim = plt.gca().get_ylim()\n",
        "                ylim = tuple(int(y) for y in lim)\n",
        "                plt.yticks(label_dict[category2][0][:], label_dict[category2][1][:])\n",
        "\n",
        "        plt.savefig('kde.png')\n",
        "        plt.axis('off')\n",
        "        plt.savefig('kde_both.png', bbox_inches='tight', pad_inches=0)\n",
        "        plt.close()\n",
        "        # Visualizza il grafico creato\n",
        "        img = imageio.imread('kde.png')\n",
        "        fig, ax = plt.subplots(figsize=(12, 10))\n",
        "        ax.imshow(img)\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "        # PROXIMITY\n",
        "        x_min = np.min(combined[feature1])\n",
        "        x_max = np.max(combined[feature1])\n",
        "        y_min = np.min(combined[feature2])\n",
        "        y_max = np.max(combined[feature2])\n",
        "\n",
        "        width = x_max - x_min\n",
        "        height = y_max - y_min\n",
        "        image_diagonal = np.sqrt(width**2 + height**2)\n",
        "\n",
        "\n",
        "        # Calculate distances between contour lines for accepted loans\n",
        "        distances_accepted = self.calculate_distances(ax1.collections)\n",
        "\n",
        "        # Normalize distances with image diagonal\n",
        "        normalized_distances = np.array(distances_accepted) / image_diagonal\n",
        "        min_distance = np.min(normalized_distances)\n",
        "        max_distance = np.max(normalized_distances)\n",
        "\n",
        "        # Calculate mean of distances\n",
        "        average_distance = np.mean(normalized_distances)\n",
        "\n",
        "        # Calculate standard deviation of distances\n",
        "        std_distance = np.std(normalized_distances)\n",
        "\n",
        "        # Calculate percentiles of normalized distances\n",
        "        min_ratio = np.percentile(normalized_distances, 5)\n",
        "        max_ratio = np.percentile(normalized_distances, 95)\n",
        "\n",
        "        # Calculate percentages of points too close or too distant to their nearest neighbors\n",
        "        too_close = len(np.where(normalized_distances <= min_ratio)[0]) / len(distances_accepted)\n",
        "        too_distance = len(np.where(normalized_distances >= max_ratio)[0]) / len(distances_accepted)\n",
        "\n",
        "        # Calculate proximity score\n",
        "        proximity_accepted_score = (min_distance + max_distance + average_distance + (1 - too_close) + (1 - too_distance) +\n",
        "                                    std_distance) / 6\n",
        "\n",
        "        # Calculate distances between contour lines for refused loans\n",
        "        distances_refused = self.calculate_distances(ax2.collections)\n",
        "\n",
        "        # Normalize distances with image diagonal\n",
        "        normalized_distances = np.array(distances_refused) / image_diagonal\n",
        "        min_distance = np.min(normalized_distances)\n",
        "        max_distance = np.max(normalized_distances)\n",
        "\n",
        "        # Calculate mean of distances\n",
        "        average_distance = np.mean(normalized_distances)\n",
        "\n",
        "        # Calculate standard deviation of distances\n",
        "        std_distance = np.std(normalized_distances)\n",
        "\n",
        "        # Calculate percentiles of normalized distances\n",
        "        min_ratio = np.percentile(normalized_distances, 5)\n",
        "        max_ratio = np.percentile(normalized_distances, 95)\n",
        "\n",
        "        # Calculate percentages of points too close or too distant to their nearest neighbors\n",
        "        too_close = len(np.where(normalized_distances <= min_ratio)[0]) / len(distances_refused)\n",
        "        too_distance = len(np.where(normalized_distances >= max_ratio)[0]) / len(distances_refused)\n",
        "\n",
        "        # Calculate proximity score\n",
        "        proximity_refused_score = (min_distance + max_distance + average_distance + (1 - too_close) + (1 - too_distance) +\n",
        "                                    std_distance) / 6\n",
        "\n",
        "        proximity_score = (proximity_accepted_score + proximity_refused_score) / 2\n",
        "\n",
        "        # CLOSURE\n",
        "        # Calculate distances between contour lines for accepted loans\n",
        "        areas_accepted = self.calculate_areas(ax1.collections)\n",
        "\n",
        "        # Calculate distances between contour lines for refused loans\n",
        "        areas_refused = self.calculate_areas(ax2.collections)\n",
        "\n",
        "        # Calculate the total area of the image\n",
        "        total_area = width * height\n",
        "\n",
        "        # Calculate the largest area for accepted and rejected separately\n",
        "        largest_area_accepted = max(areas_accepted)\n",
        "        largest_area_rejected = max(areas_refused)\n",
        "\n",
        "        # Calculate standard deviation for blu and red separately\n",
        "        std_dev_accepted = np.std(areas_accepted)/((max(areas_accepted)-min(areas_accepted))/2) if len(areas_accepted) > 1 else 1\n",
        "        std_dev_rejected = np.std(areas_refused)/((max(areas_refused)-min(areas_refused))/2) if len(areas_refused) > 1 else 1\n",
        "\n",
        "        # Calculate closure score for accepted and rejected separately\n",
        "        closure_score_accepted = (largest_area_accepted / total_area) * std_dev_accepted\n",
        "        closure_score_rejected = (largest_area_rejected / total_area) * std_dev_rejected\n",
        "\n",
        "        closure_score = (closure_score_accepted + closure_score_rejected)/2\n",
        "\n",
        "        # SYMMETRY\n",
        "        symmetry_score = calculate_symmetry('kde_both.png')\n",
        "\n",
        "        # OVERLAPPING\n",
        "        overlap_score = calculate_overlap('kde_accepted.png', 'kde_refused.png', 'white', 'white')\n",
        "\n",
        "        # Calculate metrics for the plot\n",
        "        closure_value = closure_score\n",
        "        proximity_value = proximity_score\n",
        "        symmetry_value = symmetry_score\n",
        "        overlap_value = overlap_score\n",
        "\n",
        "        total_score=closure_value+proximity_value+symmetry_value\n",
        "\n",
        "        print({\n",
        "            'closure': closure_value,\n",
        "            'proximity': proximity_value,\n",
        "            'symmetry': symmetry_value,\n",
        "            'overlap': overlap_value\n",
        "        })\n",
        "\n",
        "        return [proximity_value, closure_value, symmetry_value, overlap_score]\n",
        "\n",
        "    def calculate_distances(self, contour):\n",
        "        distances = []\n",
        "        closest_points = []\n",
        "        closest_points_seg = []\n",
        "        # Trova valori massimi e minimi di x e y\n",
        "        min_x = min([seg[:, 0].min() for contour_line in contour for path in contour_line.get_paths() for seg in path.to_polygons()])\n",
        "        max_x = max([seg[:, 0].max() for contour_line in contour for path in contour_line.get_paths() for seg in path.to_polygons()])\n",
        "        min_y = min([seg[:, 1].min() for contour_line in contour for path in contour_line.get_paths() for seg in path.to_polygons()])\n",
        "        max_y = max([seg[:, 1].max() for contour_line in contour for path in contour_line.get_paths() for seg in path.to_polygons()])\n",
        "\n",
        "        # Ricalcola i valori di x e y come coordinate di un grafico di 1000x600\n",
        "        scale_factor_x = 10 *80 / (max_x - min_x)\n",
        "        scale_factor_y = 6 * 80 / (max_y - min_y)\n",
        "\n",
        "        for i in range(len(contour)-1):  # We subtract 1 here to avoid an index error on the next line\n",
        "            min_distance = float('inf')\n",
        "            closest_points_seg = []\n",
        "            for ii in range(len(contour[i].get_paths())):\n",
        "                  seg1 = contour[i].get_paths()[ii].to_polygons()[0]\n",
        "                  col_trasf_10 = seg1[:, 0] * scale_factor_x\n",
        "                  col_trasf_11 = seg1[:, 1] * scale_factor_y\n",
        "                  seg1 = np.hstack((seg1, col_trasf_10.reshape(-1, 1), col_trasf_11.reshape(-1, 1)))\n",
        "                  for jj in range(len(contour[i+1].get_paths())):\n",
        "                      seg2 = contour[i+1].get_paths()[jj].to_polygons()[0]\n",
        "                      col_trasf_20 = seg2[:, 0] * scale_factor_x\n",
        "                      col_trasf_21 = seg2[:, 1] * scale_factor_y\n",
        "                      seg2 = np.hstack((seg2, col_trasf_20.reshape(-1, 1), col_trasf_21.reshape(-1, 1)))\n",
        "                      dist_seg = cdist(seg1[:, 2:], seg2[:, 2:], 'euclidean')\n",
        "                      distance = np.min(dist_seg)  # Calculate the minimum distance between the two segments\n",
        "                      if distance < min_distance:\n",
        "                          min_distance = distance\n",
        "                          min_dist_idx = np.unravel_index(np.argmin(dist_seg), dist_seg.shape)\n",
        "                          closest_points_seg = (seg1[min_dist_idx[0]][:2], seg2[min_dist_idx[1]][:2])\n",
        "\n",
        "            if len(closest_points_seg) > 0:\n",
        "                closest_points.append(closest_points_seg)\n",
        "                distances.append(np.sqrt((closest_points_seg[0][0]-closest_points_seg[1][0])**2+(closest_points_seg[0][1]-closest_points_seg[1][1])**2))\n",
        "        return distances\n",
        "\n",
        "    def calculate_areas(self, contour):\n",
        "        areas = []\n",
        "        for i in range(len(contour)):\n",
        "            area = 0\n",
        "            for j in range(len(contour[i].get_paths())):\n",
        "                x = contour[i].get_paths()[j].to_polygons()[0][:, 0]\n",
        "                y = contour[i].get_paths()[j].to_polygons()[0][:, 1]\n",
        "                polygon = Polygon(zip(x, y))\n",
        "                area += polygon.area\n",
        "            areas.append(area)\n",
        "        return areas"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Graphs Implementation**"
      ],
      "metadata": {
        "id": "8PZvT70tVC6n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "324gMBiqKJpp"
      },
      "outputs": [],
      "source": [
        "viz2 = DataViz2(df)\n",
        "# Create a dictionary to save the metrics\n",
        "metrics_list = []\n",
        "\n",
        "# Execute the heatmap and save the metrics in the dictionary\n",
        "heatmap_metrics = viz2.heatmap(feature1, feature2)\n",
        "metrics_list.append(['heatmap.png', np.array(heatmap_metrics)])\n",
        "\n",
        "# Execute the hexbin plot and save the metrics in the dictionary\n",
        "hexbin_metrics = viz2.hexbin_plot(feature1, feature2)\n",
        "metrics_list.append(['hexbin.png', np.array(hexbin_metrics)])\n",
        "\n",
        "# Execute the histogram and save the metrics in the dictionary\n",
        "histogram_metrics = viz2.histogram(feature1, feature2)\n",
        "metrics_list.append(['histogram.png', np.array(histogram_metrics)])\n",
        "\n",
        "# Execute the scatter plot and save the metrics in the dictionary\n",
        "scatter_plot_metrics = viz2.scatter_plot(feature1, feature2)\n",
        "metrics_list.append(['scatter.png', np.array(scatter_plot_metrics)])\n",
        "\n",
        "# Execute the KDE plot and save the metrics in the dictionary\n",
        "kde_plot_metrics = viz2.kde_plot(feature1, feature2)\n",
        "metrics_list.append(['kde.png', np.array(kde_plot_metrics)])\n",
        "\n",
        "metrics_dict = np.empty((len(metrics_list), 2), dtype=object)\n",
        "for i in range(len(metrics_list)):\n",
        "    metrics_dict[i, 0] = metrics_list[i][0]\n",
        "    metrics_dict[i, 1] = metrics_list[i][1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ip4lLqREK81Z"
      },
      "source": [
        "# **MAPS elites and Coefficients Computations**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVqDl3hbK4SF"
      },
      "outputs": [],
      "source": [
        "# Check if coefficients file already exists\n",
        "if Path('best_coeff.npy').is_file():\n",
        "    coefficients = np.load('best_coeff.npy', allow_pickle=True)\n",
        "else:\n",
        "    coefficients = np.ones(len(metrics_dict[0, 1]))\n",
        "    matrix = np.empty((0, 3), dtype=object)\n",
        "    np.save('metrics_matrix.npy', matrix)\n",
        "\n",
        "order = np.zeros(metrics_dict.shape[0])\n",
        "np.save('order.npy', order)\n",
        "\n",
        "image_value = []\n",
        "for i in range(metrics_dict.shape[0]):\n",
        "    image_value.append(np.dot(metrics_dict[i, 1], coefficients))\n",
        "image_value = np.array(image_value).T\n",
        "image_value = image_value.reshape(-1, 1)\n",
        "\n",
        "image_order = np.concatenate((metrics_dict, image_value), axis=1)\n",
        "\n",
        "# Sort the new list by score, from the one with the highest value to the lowest\n",
        "image_order = sorted(image_order, key=lambda x: x[2], reverse=True)\n",
        "np.save('image_order.npy', image_order)\n",
        "\n",
        "# Border width\n",
        "border_width = 2\n",
        "\n",
        "# New dimensions of the images\n",
        "# Create a function to resize and add a border to an image\n",
        "def resize_and_add_border(image_path):\n",
        "    # Open the image with PIL and resize\n",
        "    original_image = PILImage.open(image_path)\n",
        "    new_width = original_image.size[0] // 3\n",
        "    new_height = original_image.size[1] // 3\n",
        "\n",
        "    resized_image = original_image.resize((new_width, new_height))\n",
        "\n",
        "    # Add the border using ImageDraw\n",
        "    draw = ImageDraw.Draw(resized_image)\n",
        "    draw.rectangle([0, 0, new_width, new_height], outline='black', width=border_width)\n",
        "\n",
        "    # Save the resized image with the border to a buffer\n",
        "    image_buffer = io.BytesIO()\n",
        "    resized_image.save(image_buffer, format='PNG')\n",
        "\n",
        "    # Return the binary data of the image\n",
        "    return image_buffer.getvalue()\n",
        "\n",
        "def on_image_click(path):\n",
        "\n",
        "    order = np.load('order.npy')\n",
        "\n",
        "    if path == image_path[0]:\n",
        "        order[0] = (math.sqrt(np.sum(order) * 8 + 1) - 1) / 2 + 1\n",
        "        button_widgets[0].children[0].disabled = True\n",
        "        np.save('order.npy', order)\n",
        "    elif path == image_path[1]:\n",
        "        order[1] = (math.sqrt(np.sum(order) * 8 + 1) - 1) / 2 + 1\n",
        "        button_widgets[1].children[0].disabled = True\n",
        "        np.save('order.npy', order)\n",
        "    elif path == image_path[2]:\n",
        "        order[2] = (math.sqrt(np.sum(order) * 8 + 1) - 1) / 2 + 1\n",
        "        button_widgets[2].children[0].disabled = True\n",
        "        np.save('order.npy', order)\n",
        "    elif path == image_path[3]:\n",
        "        order[3] = (math.sqrt(np.sum(order) * 8 + 1) - 1) / 2 + 1\n",
        "        button_widgets[3].children[0].disabled = True\n",
        "        np.save('order.npy', order)\n",
        "    elif path == image_path[4]:\n",
        "        order[4] = (math.sqrt(np.sum(order) * 8 + 1) - 1) / 2 + 1\n",
        "        button_widgets[4].children[0].disabled = True\n",
        "        np.save('order.npy', order)\n",
        "    display(Image(data=resize_and_add_border(path), format='png'))\n",
        "\n",
        "# Event handling function for the click on the second button\n",
        "def on_second_button_click(b):\n",
        "    for i in range(len(button_widgets)):\n",
        "        button_widgets[i].children[0].disabled = False\n",
        "        clear_output()\n",
        "        display(widgets.HBox(button_widgets))\n",
        "        order = np.zeros(metrics_dict.shape[0])\n",
        "        np.save('order.npy', order)\n",
        "\n",
        "# Event handling function for the click on the third button\n",
        "def on_third_button_click(b):\n",
        "    order = np.load('order.npy')\n",
        "    if np.sum(order) == (order.size) * (order.size + 1) / 2:\n",
        "        b.disabled = True\n",
        "        button2.disabled = True\n",
        "\n",
        "        # Converts the list of lists to an array of arrays\n",
        "        image_order = np.load('image_order.npy', allow_pickle=True)\n",
        "        image_order = np.delete(image_order, 0, 1)\n",
        "        image_order = np.delete(image_order, 1, 1)\n",
        "        arr=[]\n",
        "        for i in range(image_order.shape[0]):\n",
        "          row = np.concatenate(image_order[i], axis=0)\n",
        "          arr.append(row)\n",
        "        arr=np.array(arr)\n",
        "        order = order.reshape(-1, 1)\n",
        "        arr = np.append(arr, order, axis=1)\n",
        "        idx = np.lexsort((arr[:, 3], arr[:, 2], arr[:, 1], arr[:, 0]))\n",
        "        sorted_arr = arr[idx]\n",
        "        find = False\n",
        "        matrix = np.load('metrics_matrix.npy', allow_pickle=True)\n",
        "        for i in range(matrix.shape[0]):\n",
        "            index = np.where(np.all(matrix[:, 1][i] == np.array(sorted_arr[:, :-1])))\n",
        "            if len(index[0]) > 0:\n",
        "                find = True\n",
        "                matrix[i, 2] = matrix[i, 2] + sorted_arr[:, -1]\n",
        "                break\n",
        "        if not find:\n",
        "            new_row = [matrix.shape[0]+1, np.concatenate(sorted_arr[:, :-1]).reshape(5, 4), sorted_arr[:, -1]]\n",
        "            row_metrics = np.empty((1, 3), dtype=object)\n",
        "            row_metrics[0] = new_row\n",
        "            matrix = np.vstack((matrix, row_metrics))\n",
        "\n",
        "        np.save('metrics_matrix.npy', matrix)\n",
        "    else:\n",
        "        print(order)\n",
        "        print(\"Please complete the choices\")\n",
        "\n",
        "# Create button widgets with images\n",
        "button_widgets = []\n",
        "arr = np.array(image_order)\n",
        "image_path = arr[:, 0]\n",
        "for path in image_path:\n",
        "    image_data = resize_and_add_border(path)\n",
        "    image = PILImage.open(io.BytesIO(image_data))\n",
        "    width, height = image.size\n",
        "    image_html = f'<img src=\"data:image/png;base64,{base64.b64encode(image_data).decode()}\" width=\"{width}\" height=\"{height}\">'\n",
        "\n",
        "    button = widgets.Button(description='Select', layout=widgets.Layout(width=f'{width}px', height=f'{50}px', text_align='left'))\n",
        "    output = widgets.Output()\n",
        "    with output:\n",
        "        display(Image(data=image_data, format='png'))\n",
        "    button.on_click(lambda event, output=output, path=path: [output.clear_output(wait=True), on_image_click(path)])\n",
        "\n",
        "    button_widgets.append(widgets.VBox([button, output]))\n",
        "\n",
        "# Show buttons with images\n",
        "button2 = widgets.Button(description='Delete', layout=widgets.Layout(width='75px', height='40px', text_align='left'))\n",
        "button2.on_click(on_second_button_click)\n",
        "output = widgets.Output()\n",
        "button3 = widgets.Button(description='OK', layout=widgets.Layout(width='75px', height='40px', text_align='left'))\n",
        "button3.on_click(on_third_button_click)\n",
        "output = widgets.Output()\n",
        "button_widgets.append(widgets.VBox([button2, output]))\n",
        "button_widgets.append(widgets.VBox([button3, output]))\n",
        "display(widgets.HBox(button_widgets))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WDgsEfrvjgIB"
      },
      "outputs": [],
      "source": [
        "#np.save('matrix.npy', matrix)\n",
        "\n",
        "CONFIG = {\n",
        "    \"map_elites\": {\n",
        "        \"dim\": 4,\n",
        "        \"iters\": 500,\n",
        "        \"archive_dims\": (500, 500),\n",
        "        \"use_result_archive\": True,\n",
        "        \"is_dqd\": True,\n",
        "        \"batch_size\": 20,\n",
        "        \"archive\": {\n",
        "            \"class\": GridArchive,\n",
        "            \"kwargs\": {\n",
        "                \"threshold_min\": -np.inf\n",
        "            }\n",
        "        },\n",
        "        \"emitters\": [{\n",
        "            \"class\": EvolutionStrategyEmitter,\n",
        "            \"kwargs\": {\n",
        "                \"sigma0\": 0.5,\n",
        "                \"ranker\": \"2rd\",\n",
        "                \"selection_rule\": \"filter\",\n",
        "                \"restart_rule\": \"no_improvement\"\n",
        "            },\n",
        "            \"num_emitters\": 15\n",
        "        }],\n",
        "        \"scheduler\": {\n",
        "            \"class\": Scheduler,\n",
        "            \"kwargs\": {}\n",
        "        }\n",
        "    }\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the fit function which takes a batch of solutions as input\n",
        "def fit(solution_batch):\n",
        "    # Loading a precomputed metrics matrix from a file\n",
        "    matrix = np.load('metrics_matrix.npy', allow_pickle=True)\n",
        "\n",
        "    # Extracting dimensions from the metrics matrix\n",
        "    n_dataset, n_graph, dim = matrix.shape[0], matrix[0, 1].shape[0], matrix[0, 1].shape[1]\n",
        "\n",
        "    # Initializing variables for tracking results\n",
        "    find_sol = False\n",
        "    measure = []\n",
        "    objective = []\n",
        "    max_valid=0\n",
        "\n",
        "    # Loop over each solution in the batch\n",
        "    for _, coeff in enumerate(solution_batch):\n",
        "        # Initializing variables for tracking combinations and valid solutions\n",
        "        combinations_value = 0\n",
        "        valid_solution_count = 0\n",
        "\n",
        "        # Looping over each dataset\n",
        "        for j in range(n_dataset):\n",
        "            # Combining two matrices and sorting them based on the last column\n",
        "            arr = np.append(matrix[j, 1], matrix[j, 2].reshape(-1, 1), axis=1)\n",
        "            idx = np.argsort(arr[:, 4])\n",
        "            arr_sorted = arr[idx]\n",
        "\n",
        "            # Compute linear combinations and check for partial order satisfaction\n",
        "            combinations = np.dot(arr_sorted[:, :-1], np.array(coeff - 5).T)\n",
        "\n",
        "            for k in range(n_graph - 1):\n",
        "                # Check if only partial order is satisfied (first element at the top position)\n",
        "                if combinations[k] == np.max(combinations[-n_graph + k:]):\n",
        "                    combinations_value += n_graph * 2 - k * 2\n",
        "                elif k == 0:\n",
        "                    combinations_value = 0.1  # If the first element is not at the top, set a default value\n",
        "\n",
        "            # Counting valid solutions\n",
        "            if combinations_value != 0:\n",
        "                valid_solution_count += 1\n",
        "\n",
        "        # Update maximum valid solutions count\n",
        "        if valid_solution_count > max_valid:\n",
        "            max_valid = valid_solution_count\n",
        "\n",
        "        # Updating the find_sol variable based on the ratio of valid solutions\n",
        "        find_sol = find_sol or ((valid_solution_count / n_dataset) > 0.9)\n",
        "\n",
        "        # Computing mean and standard deviation of the coefficients\n",
        "        mean_coeff = np.mean(coeff)\n",
        "        std_coeff = np.std(coeff)\n",
        "\n",
        "        # Appending results to the measure and objective lists\n",
        "        measure.append((mean_coeff, std_coeff))\n",
        "        objective.append(combinations_value)\n",
        "\n",
        "    # If no valid solutions are found, print a message and exit\n",
        "    if not find_sol:\n",
        "        print('There are no solutions, please review metrics value!')\n",
        "        raise SystemExit\n",
        "\n",
        "    # Returning the results as NumPy arrays\n",
        "    return np.array(objective), np.array(measure)"
      ],
      "metadata": {
        "id": "CnFlFWSyOSb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_A9KeOjsxsmn"
      },
      "outputs": [],
      "source": [
        "def create_scheduler(config, algorithm, seed=None):\n",
        "    solution_dim = config[\"dim\"]\n",
        "    archive_dims = config[\"archive_dims\"]\n",
        "    learning_rate = 0.1 if \"learning_rate\" not in config[\"archive\"][\n",
        "        \"kwargs\"] else config[\"archive\"][\"kwargs\"][\"learning_rate\"]\n",
        "    use_result_archive = config[\"use_result_archive\"]\n",
        "    bounds = [(0.001, 1.), (0.001, 1.)]\n",
        "    bounds_emitter = [(0.001, 10.), (0.001, 10.), (0.001, 10.), (0.001, 10.)]\n",
        "    initial_coeff = np.ones(solution_dim)\n",
        "    mode = \"batch\"\n",
        "\n",
        "    # Create archive.\n",
        "    archive_class = config[\"archive\"][\"class\"]\n",
        "    if archive_class == GridArchive:\n",
        "        archive = archive_class(solution_dim=solution_dim,\n",
        "                                ranges=bounds,\n",
        "                                dims=archive_dims,\n",
        "                                seed=seed,\n",
        "                                **config[\"archive\"][\"kwargs\"])\n",
        "\n",
        "    else:\n",
        "        archive = archive_class(solution_dim=solution_dim,\n",
        "                                ranges=bounds,\n",
        "                                seed=seed,\n",
        "                                **config[\"archive\"][\"kwargs\"])\n",
        "\n",
        "    # Create result archive.\n",
        "    result_archive = None\n",
        "    if use_result_archive:\n",
        "        result_archive = GridArchive(solution_dim=solution_dim,\n",
        "                                     dims=archive_dims,\n",
        "                                     ranges=bounds,\n",
        "                                     seed=seed)\n",
        "\n",
        "    # Create emitters. Each emitter needs a different seed so that they do not\n",
        "    # all do the same thing, hence we create an rng here to generate seeds. The\n",
        "    # rng may be seeded with None or with a user-provided seed.\n",
        "    seed_sequence = np.random.SeedSequence(seed)\n",
        "    emitters = []\n",
        "    for e in config[\"emitters\"]:\n",
        "        emitter_class = e[\"class\"]\n",
        "        emitters += [\n",
        "            emitter_class(\n",
        "                archive,\n",
        "                x0=initial_coeff,\n",
        "                bounds=bounds_emitter,\n",
        "                **e[\"kwargs\"],\n",
        "                batch_size=config[\"batch_size\"],\n",
        "                seed=s,\n",
        "            ) for s in seed_sequence.spawn(e[\"num_emitters\"])\n",
        "        ]\n",
        "    # Create Scheduler\n",
        "    scheduler_class = config[\"scheduler\"][\"class\"]\n",
        "    scheduler = scheduler_class(archive,\n",
        "                                emitters,\n",
        "                                result_archive=result_archive,\n",
        "                                add_mode=mode,\n",
        "                                **config[\"scheduler\"][\"kwargs\"])\n",
        "    scheduler_name = scheduler.__class__.__name__\n",
        "\n",
        "    print(f\"Create {scheduler_name} for {algorithm} with learning rate \"\n",
        "          f\"{learning_rate} and add mode {mode}, using solution dim \"\n",
        "          f\"{solution_dim}, archive dims {archive_dims}, and \"\n",
        "          f\"{len(emitters)} emitters.\")\n",
        "\n",
        "    return scheduler\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3NO9F7O6xusN"
      },
      "outputs": [],
      "source": [
        "def save_heatmap(archive, heatmap_path):\n",
        "\n",
        "    if isinstance(archive, GridArchive):\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        grid_archive_heatmap(archive, vmin=0, vmax=100)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(heatmap_path)\n",
        "    elif isinstance(archive, CVTArchive):\n",
        "        plt.figure(figsize=(16, 12))\n",
        "        cvt_archive_heatmap(archive, vmin=0, vmax=100)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(heatmap_path)\n",
        "    plt.close(plt.gcf())\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xKXiPli2xwTi"
      },
      "outputs": [],
      "source": [
        "#def main\n",
        "algorithm = \"map_elites\"\n",
        "dim=None\n",
        "itrs=None\n",
        "archive_dims=None\n",
        "learning_rate=None\n",
        "es=None\n",
        "outdir=\"coeff_output\"\n",
        "log_freq=250\n",
        "seed=None\n",
        "config = copy.deepcopy(CONFIG[\"map_elites\"])\n",
        "print('config', config)\n",
        "\n",
        "# Use default dim for each algorithm.\n",
        "if dim is not None:\n",
        "    config[\"dim\"] = dim\n",
        "\n",
        "# Use default itrs for each algorithm.\n",
        "if itrs is not None:\n",
        "    config[\"iters\"] = itrs\n",
        "\n",
        "# Use default archive_dim for each algorithm.\n",
        "if archive_dims is not None:\n",
        "    config[\"archive_dims\"] = archive_dims\n",
        "\n",
        "# Use default learning_rate for each algorithm.\n",
        "if learning_rate is not None:\n",
        "    config[\"archive\"][\"kwargs\"][\"learning_rate\"] = learning_rate\n",
        "\n",
        "name = f\"{algorithm}_{config['dim']}\"\n",
        "outdir = Path(outdir)\n",
        "if not outdir.is_dir():\n",
        "    outdir.mkdir()\n",
        "\n",
        "scheduler = create_scheduler(config, algorithm, seed=seed)\n",
        "result_archive = scheduler.result_archive\n",
        "is_dqd = config[\"is_dqd\"]\n",
        "itrs = config[\"iters\"]\n",
        "metrics = {\n",
        "    \"QD Score\": {\n",
        "        \"x\": [0],\n",
        "        \"y\": [0.0],\n",
        "    },\n",
        "    \"Archive Coverage\": {\n",
        "        \"x\": [0],\n",
        "        \"y\": [0.0],\n",
        "    },\n",
        "}\n",
        "\n",
        "non_logging_time = 0.0\n",
        "save_heatmap(result_archive, str(outdir / f\"{name}_heatmap_{0:05d}.png\"))\n",
        "\n",
        "existing_coefficients = None  # Assume no existing coefficients at the beginning\n",
        "\n",
        "# Check if coefficients file already exists\n",
        "coefficients_file_path = outdir / f\"{name}_existing_coefficients.txt\"\n",
        "if coefficients_file_path.exists():\n",
        "    # Read existing coefficients from file\n",
        "    with coefficients_file_path.open(\"r\") as existing_coeff_file:\n",
        "        existing_coefficients = [list(map(float, line.split())) for line in existing_coeff_file.readlines()]\n",
        "\n",
        "# Use existing coefficients if available, otherwise generate new ones\n",
        "initial_coefficients = existing_coefficients or np.ones(dim)  # Assuming dim is the dimension of the coefficients\n",
        "\n",
        "\n",
        "for itr in tqdm.trange(1, itrs + 1):\n",
        "    itr_start = time.time()\n",
        "\n",
        "    # Call your fit function to get objective values, measures, and diversity measure.\n",
        "    solution_batch = scheduler.ask()\n",
        "\n",
        "    objective_batch, measure_batch = fit(solution_batch)\n",
        "\n",
        "    # Call tell to provide feedback on the evaluated solutions\n",
        "    scheduler.tell(objective_batch, measure_batch)\n",
        "\n",
        "    # Record the QD Score in the metrics dictionary.\n",
        "    # Logging and output.\n",
        "    final_itr = itr == itrs\n",
        "    if itr % log_freq == 0 or final_itr:\n",
        "        if final_itr:\n",
        "            result_archive.data(return_type='pandas'\n",
        "            ).to_csv(outdir / f\"{name}_archive.csv\")\n",
        "\n",
        "        # Record the QD Score in the metrics dictionary.\n",
        "        metrics[\"QD Score\"][\"x\"].append(itr)\n",
        "        metrics[\"QD Score\"][\"y\"].append(result_archive.stats.qd_score)\n",
        "        metrics[\"Archive Coverage\"][\"x\"].append(itr)\n",
        "        metrics[\"Archive Coverage\"][\"y\"].append(\n",
        "            result_archive.stats.coverage)\n",
        "        tqdm.tqdm.write(\n",
        "            f\"Iteration {itr} | Archive Coverage: \"\n",
        "            f\"{metrics['Archive Coverage']['y'][-1] * 100:.3f}% \"\n",
        "            f\"QD Score: {metrics['QD Score']['y'][-1]:.3f}\")\n",
        "\n",
        "        save_heatmap(result_archive,\n",
        "                      str(outdir / f\"{name}_heatmap_{itr:05d}.png\"))\n",
        "print(result_archive.best_elite[\"solution\"]-5)\n",
        "np.save('best_coeff.npy', result_archive.best_elite[\"solution\"]-5)\n",
        "# Plot metrics.\n",
        "print(f\"Algorithm Time (Excludes Logging and Setup): {non_logging_time}s\")\n",
        "for metric, values in metrics.items():\n",
        "    plt.plot(values[\"x\"], values[\"y\"])\n",
        "    plt.title(metric)\n",
        "    plt.xlabel(\"Iteration\")\n",
        "    plt.savefig(\n",
        "        str(outdir / f\"{name}_{metric.lower().replace(' ', '_')}.png\"))\n",
        "    plt.clf()\n",
        "with (outdir / f\"{name}_metrics.json\").open(\"w\") as file:\n",
        "    json.dump(metrics, file, indent=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Best Graph**"
      ],
      "metadata": {
        "id": "basynpusVvTk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the best graph according to your preferences\n",
        "\n",
        "# Check if coefficients file already exists\n",
        "if Path('best_coeff.npy').is_file():\n",
        "    coefficients = np.load('best_coeff.npy', allow_pickle=True)\n",
        "\n",
        "    order = np.zeros(metrics_dict.shape[0])\n",
        "    np.save('order.npy', order)\n",
        "\n",
        "    image_value = []\n",
        "    for i in range(metrics_dict.shape[0]):\n",
        "        image_value.append(np.dot(metrics_dict[i, 1], coefficients))\n",
        "    image_value = np.array(image_value).T\n",
        "    image_value = image_value.reshape(-1, 1)\n",
        "\n",
        "    image_order = np.concatenate((metrics_dict, image_value), axis=1)\n",
        "\n",
        "    # The new list is sorted according to score, from the one with the highest to the lowest value\n",
        "    image_order = sorted(image_order, key=lambda x: x[2], reverse=True)\n",
        "\n",
        "    image = imageio.imread(image_order[0][0])\n",
        "    fig, ax = plt.subplots(figsize=(12, 10))\n",
        "\n",
        "    # View image\n",
        "    ax.imshow(image)\n",
        "    ax.axis('off')  # Hide the axes\n",
        "    plt.show()\n",
        "else:\n",
        "    print('Sorry, there are no previous choices')"
      ],
      "metadata": {
        "id": "CMQpuT13ashr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_2Kz5EkK2vj"
      },
      "source": [
        "# **Bias Detection**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2NqbASS7G365"
      },
      "outputs": [],
      "source": [
        "####### USED ONLY FOR EXAMPLE #########\n",
        "import joblib\n",
        "model = joblib.load('model.joblib')\n",
        "df = pd.read_csv('trial_dataset.csv')\n",
        "feature1 = 'Age'\n",
        "target_column = 'Loan Approval'\n",
        "#######################################\n",
        "\n",
        "# Create a histogram of the data\n",
        "fig = px.histogram(df, x=feature1, color=target_column)\n",
        "\n",
        "# Define the Dash app\n",
        "app = Dash(__name__)\n",
        "\n",
        "# Variable to store selected data\n",
        "selected_data = None\n",
        "\n",
        "# Layout of the app\n",
        "app.layout = html.Div([\n",
        "    dcc.Graph(figure=fig, id='histogram-plot'),\n",
        "    html.Div(id='selected-data-output'),\n",
        "])\n",
        "\n",
        "# Callback to update the selected data\n",
        "@app.callback(\n",
        "    Output('selected-data-output', 'children'),\n",
        "    [Input('histogram-plot', 'selectedData')],\n",
        "    [State('selected-data-output', 'children')]\n",
        ")\n",
        "def display_selected_data(selectedData, current_output):\n",
        "    global selected_data\n",
        "\n",
        "    if selectedData is not None:\n",
        "        selected_range = selectedData['range']['x']\n",
        "        selected_data = df[(df[feature1] >= selected_range[0]) & (df[feature1] <= selected_range[1])]\n",
        "        fig_sel = px.histogram(selected_data, x=feature1, color=target_column)\n",
        "        fig_sel.show()\n",
        "        return f'Selected data: {selected_data.to_dict(orient=\"records\")}'\n",
        "    else:\n",
        "        return 'No data selected'\n",
        "\n",
        "# Run the app\n",
        "app.run_server(mode='inline')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJ-02ACd8C7W"
      },
      "outputs": [],
      "source": [
        "from plotly.subplots import make_subplots\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "mean = selected_data.mean()\n",
        "std = selected_data.std()\n",
        "index_sel = selected_data.index\n",
        "def perturb(x, mean, std, factor):\n",
        "  # This function adds random noise to the x series, keeping the mean and std value the same\n",
        "  # The factor parameter controls the amplitude of the noise, the higher it is the bigger the noise\n",
        "  noise = np.random.normal(0, std * factor, size=len(x))\n",
        "  return x + noise - (noise.mean() - mean)\n",
        "\n",
        "gender_modified = []\n",
        "for i in range(df.shape[0]):\n",
        "    gender_modified.append(1 - df['Gender'].iloc[i])\n",
        "gender_modified = pd.DataFrame(gender_modified) #, columns=['Gender_n'])\n",
        "# Assuming 'target_column' is the name of your target column\n",
        "X_modified = df.drop(columns=[target_column])\n",
        "X_modified['Gender'] = gender_modified\n",
        "X_modified.to_csv('Xmodified.csv',index=False)\n",
        "y_modified = model.predict(X_modified)\n",
        "y_modified = pd.DataFrame(y_modified, columns=[target_column])\n",
        "y_modified.to_csv('ymodified.csv',index=False)\n",
        "X_modified['Loan Approval'] = y_modified\n",
        "\n",
        "category_order = [\"blue\", \"red\"]\n",
        "\n",
        "# Creates side-by-side histogram charts with equal y-axis\n",
        "fig1 = px.histogram(df.iloc[index_sel],\n",
        "                    x=feature1,\n",
        "                    color=target_column,\n",
        "                    color_discrete_map={0: \"red\", 1: \"blue\"},\n",
        "                    category_orders={target_column: [1, 0], 'color': category_order})\n",
        "fig2 = px.histogram(X_modified.iloc[index_sel],\n",
        "                    x=feature1,\n",
        "                    color=target_column,\n",
        "                    color_discrete_map={0: \"red\", 1: \"blue\"},\n",
        "                    category_orders={target_column: [1, 0], 'color': category_order})\n",
        "\n",
        "# Show figure\n",
        "fig1.show()\n",
        "fig2.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}